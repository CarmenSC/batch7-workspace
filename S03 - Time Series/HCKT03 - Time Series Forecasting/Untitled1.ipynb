{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0da26aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40cbfa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/djl_jn195h7gym2_kn27310c0000gn/T/ipykernel_7933/3925681534.py:7: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  data['date'] = pd.to_datetime(data['date'], infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "data_path='data/data.csv'\n",
    "# Reload the data to start fresh\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Convert 'date' to datetime\n",
    "\n",
    "data['date'] = pd.to_datetime(data['date'], infer_datetime_format=True)\n",
    "\n",
    "# Set date as index\n",
    "\n",
    "\n",
    "data = data.set_index('date').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e074e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/djl_jn195h7gym2_kn27310c0000gn/T/ipykernel_7933/115459068.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_for_imputation['power'] = knn_imputer.fit_transform(data_for_imputation[['power']])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(power    168\n",
       " exog     259\n",
       " dtype: int64,\n",
       "                      power    exog\n",
       " date                              \n",
       " 2023-10-15 19:00:00    NaN  10.820\n",
       " 2023-10-15 20:00:00    NaN  10.619\n",
       " 2023-10-15 21:00:00    NaN   9.535\n",
       " 2023-10-15 22:00:00    NaN  10.031\n",
       " 2023-10-15 23:00:00    NaN   9.464)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Separate the last 168 rows for prediction\n",
    "data_for_imputation = data.iloc[:-168, :]\n",
    "data_to_predict = data.iloc[-168:, :]\n",
    "\n",
    "# Impute missing values in 'exog' for the whole dataset\n",
    "data['exog'] = knn_imputer.fit_transform(data[['exog']])\n",
    "\n",
    "# Impute missing values in 'power' except for the last 168 values\n",
    "data_for_imputation['power'] = knn_imputer.fit_transform(data_for_imputation[['power']])\n",
    "\n",
    "# Combine the data back\n",
    "data_imputed = pd.concat([data_for_imputation, data_to_predict])\n",
    "\n",
    "# Check if there are any missing values left in 'exog' and 'power' (except for the last 168 values)\n",
    "data_imputed.isnull().sum(), data_imputed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fbfa462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dk/djl_jn195h7gym2_kn27310c0000gn/T/ipykernel_7933/1762420660.py:36: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_.fillna(method='ffill', inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power</th>\n",
       "      <th>exog</th>\n",
       "      <th>lagged_1</th>\n",
       "      <th>lagged_2</th>\n",
       "      <th>lagged_3</th>\n",
       "      <th>diff_1</th>\n",
       "      <th>rolling_mean</th>\n",
       "      <th>rolling_min</th>\n",
       "      <th>rolling_max</th>\n",
       "      <th>rolling_std</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>holidays</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-20 03:00:00</th>\n",
       "      <td>984.234</td>\n",
       "      <td>11.025</td>\n",
       "      <td>928.813</td>\n",
       "      <td>952.705</td>\n",
       "      <td>976.930</td>\n",
       "      <td>55.421</td>\n",
       "      <td>960.670500</td>\n",
       "      <td>928.813</td>\n",
       "      <td>984.234</td>\n",
       "      <td>25.152597</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20 04:00:00</th>\n",
       "      <td>1002.113</td>\n",
       "      <td>11.196</td>\n",
       "      <td>984.234</td>\n",
       "      <td>928.813</td>\n",
       "      <td>952.705</td>\n",
       "      <td>17.879</td>\n",
       "      <td>968.959000</td>\n",
       "      <td>928.813</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>28.600455</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20 05:00:00</th>\n",
       "      <td>996.167</td>\n",
       "      <td>11.414</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>984.234</td>\n",
       "      <td>928.813</td>\n",
       "      <td>-5.946</td>\n",
       "      <td>973.493667</td>\n",
       "      <td>928.813</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>27.888492</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20 06:00:00</th>\n",
       "      <td>970.203</td>\n",
       "      <td>11.382</td>\n",
       "      <td>996.167</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>984.234</td>\n",
       "      <td>-25.964</td>\n",
       "      <td>973.023571</td>\n",
       "      <td>928.813</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>25.488957</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-20 07:00:00</th>\n",
       "      <td>890.502</td>\n",
       "      <td>10.974</td>\n",
       "      <td>970.203</td>\n",
       "      <td>996.167</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>-79.701</td>\n",
       "      <td>962.708375</td>\n",
       "      <td>890.502</td>\n",
       "      <td>1002.113</td>\n",
       "      <td>37.524668</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        power    exog  lagged_1  lagged_2  lagged_3  diff_1  \\\n",
       "date                                                                          \n",
       "2020-09-20 03:00:00   984.234  11.025   928.813   952.705   976.930  55.421   \n",
       "2020-09-20 04:00:00  1002.113  11.196   984.234   928.813   952.705  17.879   \n",
       "2020-09-20 05:00:00   996.167  11.414  1002.113   984.234   928.813  -5.946   \n",
       "2020-09-20 06:00:00   970.203  11.382   996.167  1002.113   984.234 -25.964   \n",
       "2020-09-20 07:00:00   890.502  10.974   970.203   996.167  1002.113 -79.701   \n",
       "\n",
       "                     rolling_mean  rolling_min  rolling_max  rolling_std  \\\n",
       "date                                                                       \n",
       "2020-09-20 03:00:00    960.670500      928.813      984.234    25.152597   \n",
       "2020-09-20 04:00:00    968.959000      928.813     1002.113    28.600455   \n",
       "2020-09-20 05:00:00    973.493667      928.813     1002.113    27.888492   \n",
       "2020-09-20 06:00:00    973.023571      928.813     1002.113    25.488957   \n",
       "2020-09-20 07:00:00    962.708375      890.502     1002.113    37.524668   \n",
       "\n",
       "                     sin_weekday  cos_weekday  sin_month     cos_month  \\\n",
       "date                                                                     \n",
       "2020-09-20 03:00:00    -0.781831      0.62349       -1.0 -1.836970e-16   \n",
       "2020-09-20 04:00:00    -0.781831      0.62349       -1.0 -1.836970e-16   \n",
       "2020-09-20 05:00:00    -0.781831      0.62349       -1.0 -1.836970e-16   \n",
       "2020-09-20 06:00:00    -0.781831      0.62349       -1.0 -1.836970e-16   \n",
       "2020-09-20 07:00:00    -0.781831      0.62349       -1.0 -1.836970e-16   \n",
       "\n",
       "                     holidays  \n",
       "date                           \n",
       "2020-09-20 03:00:00       0.0  \n",
       "2020-09-20 04:00:00       0.0  \n",
       "2020-09-20 05:00:00       0.0  \n",
       "2020-09-20 06:00:00       0.0  \n",
       "2020-09-20 07:00:00       0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "# Define the feature engineering function\n",
    "def build_some_features(df_, target, num_periods_lagged=1, num_periods_diffed=0, weekday=False, month=False, rolling=[], holidays=False):\n",
    "    \"\"\"\n",
    "    Builds some features by calculating differences between periods.\n",
    "    \"\"\"\n",
    "    df_ = df_.copy()\n",
    "            \n",
    "    for i in range(1, num_periods_lagged+1):\n",
    "        df_['lagged_%s' % str(i)] = df_[target].shift(i)\n",
    "        \n",
    "    for i in range(1, num_periods_diffed+1):\n",
    "        df_['diff_%s' % str(i)] = df_[target].diff(i)\n",
    "    \n",
    "    for stat in rolling:\n",
    "        if stat:\n",
    "            df_['rolling_mean'] = df_[target].rolling('7D').mean()\n",
    "            df_['rolling_min'] = df_[target].rolling('7D').min()\n",
    "            df_['rolling_max'] = df_[target].rolling('7D').max()\n",
    "            df_['rolling_std'] = df_[target].rolling('7D').std()\n",
    "        \n",
    "    if weekday:\n",
    "        df_['sin_weekday'] = np.sin(2 * np.pi * df_.index.weekday / 7)\n",
    "        df_['cos_weekday'] = np.cos(2 * np.pi * df_.index.weekday / 7)\n",
    "        \n",
    "    if month:\n",
    "        df_['sin_month'] = np.sin(2 * np.pi * df_.index.month / 12)\n",
    "        df_['cos_month'] = np.cos(2 * np.pi * df_.index.month / 12)\n",
    "        \n",
    "    if holidays:\n",
    "        holidays = df_[((df_.index.month == 12) & (df_.index.day == 25))\n",
    "                       | ((df_.index.month == 1) & (df_.index.day == 1))][target]\n",
    "        df_['holidays'] = holidays + 1\n",
    "        df_['holidays'].fillna(0, inplace=True)\n",
    "        \n",
    "    df_.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    return df_\n",
    "\n",
    "# Apply feature engineering\n",
    "features_to_roll = [np.mean, np.min, np.max, np.std]\n",
    "data_fe = build_some_features(data, 'power', num_periods_lagged=3, num_periods_diffed=1, weekday=True, month=True, rolling=features_to_roll, holidays=True)\n",
    "\n",
    "# Drop NA values generated by lag/diff operations\n",
    "data_fe.dropna(inplace=True)\n",
    "\n",
    "# Preview the engineered features\n",
    "data_fe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "effe1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Preparing the data for training and prediction\n",
    "# Drop rows with NaN in the target column 'power' for training data\n",
    "train_df = data_fe.iloc[:-168].dropna(subset=['power'])\n",
    "predict_df = data_fe.iloc[-168:]  # Data for prediction\n",
    "\n",
    "# Define features and target\n",
    "X_train = train_df.drop('power', axis=1)\n",
    "y_train = train_df['power']\n",
    "X_predict = predict_df.drop('power', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23d69f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([710.53153097, 710.53153097, 710.53153097, 710.53153097,\n",
       "       710.53153097])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Preparing the data for training and prediction\n",
    "# Drop rows with NaN in the target column 'power' for training data\n",
    "train_df = data_fe.iloc[:-168].dropna(subset=['power'])\n",
    "predict_df = data_fe.iloc[-168:]  # Data for prediction\n",
    "\n",
    "# Define features and target\n",
    "X_train = train_df.drop('power', axis=1)\n",
    "y_train = train_df['power']\n",
    "X_predict = predict_df.drop('power', axis=1)\n",
    "\n",
    "# Initialize the GradientBoostingRegressor model\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Train the model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict the last 168 missing values of 'power'\n",
    "power_predictions = gb_regressor.predict(X_predict)\n",
    "\n",
    "# As we don't have true values for the last 168 hours, we skip direct MAE calculation.\n",
    "# Normally, you'd calculate it like this if you had true values:\n",
    "# mae = mean_absolute_error(y_true, power_predictions)\n",
    "\n",
    "# Insert the predicted values back into the dataset\n",
    "data_fe.loc[data_fe.index[-168:], 'power'] = power_predictions\n",
    "\n",
    "# Display the first few predicted values\n",
    "power_predictions[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc83b193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([710.53153097, 710.53153097, 710.53153097, 710.53153097,\n",
       "       710.53153097])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply forward fill to handle NaNs resulting from feature engineering\n",
    "data_fe_filled = data_fe.ffill().bfill()  # Forward fill followed by backward fill to cover all NaNs\n",
    "\n",
    "# Prepare the data again for training and prediction\n",
    "X_train_filled = data_fe_filled.iloc[:-168].drop('power', axis=1)  # Exclude last 168 hours for features\n",
    "y_train_filled = data_fe_filled.iloc[:-168]['power']  # Exclude last 168 hours for target\n",
    "X_predict_filled = data_fe_filled.iloc[-168:].drop('power', axis=1)  # Features for prediction\n",
    "\n",
    "# Re-initialize and train the GradientBoostingRegressor model\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "gb_regressor.fit(X_train_filled, y_train_filled)\n",
    "\n",
    "# Re-predict the last 168 missing values of 'power'\n",
    "power_predictions_filled = gb_regressor.predict(X_predict_filled)\n",
    "\n",
    "# Insert the predicted values back into the dataset\n",
    "data_fe_filled.loc[data_fe_filled.index[-168:], 'power'] = power_predictions_filled\n",
    "\n",
    "# Display the first few predicted values as a check\n",
    "power_predictions_filled[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "376c363d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [26733, 168]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rmse_gb \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m rmse_gb\n\u001b[1;32m      5\u001b[0m mae_gb \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_train, y_pred_train)\n",
      "File \u001b[0;32m~/.pyenv/versions/S03_BLU06/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/S03_BLU06/lib/python3.10/site-packages/sklearn/metrics/_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    416\u001b[0m ):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.pyenv/versions/S03_BLU06/lib/python3.10/site-packages/sklearn/metrics/_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    101\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/S03_BLU06/lib/python3.10/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [26733, 168]"
     ]
    }
   ],
   "source": [
    "rmse_gb = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "\n",
    "rmse_gb\n",
    "\n",
    "mae_gb = mean_absolute_error(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0aa8fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.994093059455155, 8.597240482209516)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_gb,mae_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5de27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a932770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
