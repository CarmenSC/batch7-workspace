{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a3dc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f881c7002cca9b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU09 - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e125f899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:27.064912Z",
     "start_time": "2024-04-07T15:06:22.680814Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efa4dd0f5a58872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing needed packages here\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from utils import remove_punctuation, remove_stopwords\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.sha256(json.dumps(str(s)).encode()).hexdigest()\n",
    "\n",
    "cpu_count = int(os.cpu_count()) if os.cpu_count() != None else 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c4d9f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:27.932445Z",
     "start_time": "2024-04-07T15:06:27.067155Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62fce116d684ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Intro: Method Man w/ sample] + (Sunny valenti...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Sean Paul:]. Aye. It's Sean Paul 'long side. ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You've changed your tune. many times since we'...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got all these J's rolled up. And got all the...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Look I'm standing naked before you. Don't you ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lyrics    Genre\n",
       "0  [Intro: Method Man w/ sample] + (Sunny valenti...  Hip Hop\n",
       "1  [Sean Paul:]. Aye. It's Sean Paul 'long side. ...      Pop\n",
       "2  You've changed your tune. many times since we'...     Rock\n",
       "3  I got all these J's rolled up. And got all the...  Hip Hop\n",
       "4  Look I'm standing naked before you. Don't you ...     Rock"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = \"data/train.csv\"\n",
    "test_path = \"data/test.csv\"\n",
    "df_train_base = pd.read_csv(train_path)\n",
    "df_test_base = pd.read_csv(test_path)\n",
    "\n",
    "df_train_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51082b8f180b345",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:28.630536Z",
     "start_time": "2024-04-07T15:06:27.929606Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_vectorized = pd.read_csv(\"data/train_vectorized.csv\")\n",
    "df_test_vectorized = pd.read_csv(\"data/test_vectorized.csv\")\n",
    "df_train_vectorized = df_train_vectorized.drop(columns=\"Unnamed: 0\")\n",
    "df_test_vectorized = df_test_vectorized.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837892683b91bb66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:28.853965Z",
     "start_time": "2024-04-07T15:06:28.631937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>feature491</th>\n",
       "      <th>feature492</th>\n",
       "      <th>feature493</th>\n",
       "      <th>feature494</th>\n",
       "      <th>feature495</th>\n",
       "      <th>feature496</th>\n",
       "      <th>feature497</th>\n",
       "      <th>feature498</th>\n",
       "      <th>feature499</th>\n",
       "      <th>feature500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064383</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.431529</td>\n",
       "      <td>3.391720</td>\n",
       "      <td>0.011031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027639</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>0.328032</td>\n",
       "      <td>3.145129</td>\n",
       "      <td>0.340005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.12957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>0.348101</td>\n",
       "      <td>3.572785</td>\n",
       "      <td>0.140278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096320</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>0.282443</td>\n",
       "      <td>3.396947</td>\n",
       "      <td>0.014360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.047215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.408867</td>\n",
       "      <td>3.162562</td>\n",
       "      <td>0.387857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0       0.0       0.0       0.0  0.000000       0.0       0.0       0.0   \n",
       "1       0.0       0.0       0.0  0.000000       0.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0  0.000000       0.0       0.0       0.0   \n",
       "3       0.0       0.0       0.0  0.040328       0.0       0.0       0.0   \n",
       "4       0.0       0.0       0.0  0.000000       0.0       0.0       0.0   \n",
       "\n",
       "   feature8  feature9  feature10  ...  feature491  feature492  feature493  \\\n",
       "0  0.000000  0.000000        0.0  ...    0.064383    0.016288     0.00000   \n",
       "1  0.000000  0.040077        0.0  ...    0.027639    0.020977     0.00000   \n",
       "2  0.000000  0.000000        0.0  ...    0.000000    0.000000     0.12957   \n",
       "3  0.000000  0.036805        0.0  ...    0.000000    0.096320     0.00000   \n",
       "4  0.047215  0.000000        0.0  ...    0.000000    0.000000     0.00000   \n",
       "\n",
       "   feature494  feature495  feature496  feature497  feature498  feature499  \\\n",
       "0         0.0    0.025035         0.0       628.0    0.431529    3.391720   \n",
       "1         0.0    0.000000         0.0       503.0    0.328032    3.145129   \n",
       "2         0.0    0.000000         0.0       316.0    0.348101    3.572785   \n",
       "3         0.0    0.000000         0.0       524.0    0.282443    3.396947   \n",
       "4         0.0    0.000000         0.0       203.0    0.408867    3.162562   \n",
       "\n",
       "   feature500  \n",
       "0    0.011031  \n",
       "1    0.340005  \n",
       "2    0.140278  \n",
       "3    0.014360  \n",
       "4    0.387857  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_vectorized_carmen = pd.read_parquet(\"data/features_carmen.parquet\", engine=\"fastparquet\")\n",
    "df_train_vectorized_carmen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b44e2ac4676a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:28.855546Z",
     "start_time": "2024-04-07T15:06:28.850767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3354, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12eef446f9b2e1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:28.862192Z",
     "start_time": "2024-04-07T15:06:28.855094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Intro: Method Man w/ sample] + (Sunny valenti...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Sean Paul:]. Aye. It's Sean Paul 'long side. ...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You've changed your tune. many times since we'...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I got all these J's rolled up. And got all the...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Look I'm standing naked before you. Don't you ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39048</th>\n",
       "      <td>It's the end of the movie. And your story's be...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39049</th>\n",
       "      <td>Uh oh. You live in this lane. Your body's insa...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39050</th>\n",
       "      <td>Shawty swing my way put that ass all in my fac...</td>\n",
       "      <td>Hip Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39051</th>\n",
       "      <td>Born in caught out. Care out fear in. Gear in ...</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39052</th>\n",
       "      <td>Going far Getting nowhere. Going far The way y...</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39053 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Lyrics    Genre\n",
       "0      [Intro: Method Man w/ sample] + (Sunny valenti...  Hip Hop\n",
       "1      [Sean Paul:]. Aye. It's Sean Paul 'long side. ...      Pop\n",
       "2      You've changed your tune. many times since we'...     Rock\n",
       "3      I got all these J's rolled up. And got all the...  Hip Hop\n",
       "4      Look I'm standing naked before you. Don't you ...     Rock\n",
       "...                                                  ...      ...\n",
       "39048  It's the end of the movie. And your story's be...      Pop\n",
       "39049  Uh oh. You live in this lane. Your body's insa...  Hip Hop\n",
       "39050  Shawty swing my way put that ass all in my fac...  Hip Hop\n",
       "39051  Born in caught out. Care out fear in. Gear in ...     Rock\n",
       "39052  Going far Getting nowhere. Going far The way y...      Pop\n",
       "\n",
       "[39053 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train_base.copy()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54a289d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.140295Z",
     "start_time": "2024-04-07T15:06:28.863339Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb489232f7a726b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6313691e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.141379Z",
     "start_time": "2024-04-07T15:06:30.138344Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f6782b20750823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# docs = list(tqdm(nlp.pipe(df_train[\"Lyrics\"], batch_size=20, n_process=cpu_count-1), total=len(df_train[\"Lyrics\"])))\n",
    "# docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb6a3fed5efb9eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.153808Z",
     "start_time": "2024-04-07T15:06:30.141511Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     df_train_base.drop(columns=['Genre']), \n",
    "#     df_train_base['Genre'], \n",
    "#     test_size=0.2, \n",
    "#     random_state=42,\n",
    "#     stratify=df_train_base['Genre']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925e08bbd4a966e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.154029Z",
     "start_time": "2024-04-07T15:06:30.149437Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81be9195226e88c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.162363Z",
     "start_time": "2024-04-07T15:06:30.155434Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall(y_test, y_pred):\n",
    "    \"\"\"Returns the precision and recall of the helpfulness class (label = 1)\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        y_pred (Series): Predictions corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score of the helpfulness class (1) on the test data\n",
    "        recall (float): The recall score of the helpfulness class (1) on the test data\n",
    "    \"\"\"\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a49e836d8b20fdaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.166942Z",
     "start_time": "2024-04-07T15:06:30.159519Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"Returns a fitted Multinomial Naive Bayes model, the predictions on the test set\n",
    "    and the precision and recall scores for these predictions\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Vectorized text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test_vec (Series): Vectorized text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        clf (MultinomialNB): MultinomialNB classifier fitted to the vectorized training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "        precision (float): The precision score of the helpfulness class (1) on the test data\n",
    "        recall (float): The recall score of the helpfulness class (1) on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    clf =  MultinomialNB()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    precision, recall = get_precision_recall(y_test, y_pred)\n",
    "\n",
    "    return clf, y_pred, precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fab17d2059febf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.167748Z",
     "start_time": "2024-04-07T15:06:30.164304Z"
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a691cf1d2bb1ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:06:30.638582Z",
     "start_time": "2024-04-07T15:06:30.631141Z"
    }
   },
   "outputs": [],
   "source": [
    "def average_word_length(text):\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words) if words else 0\n",
    "\n",
    "def average_sentence_length(doc):\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sum(len(sent.split()) for sent in sentences) / len(sentences) if sentences else 0\n",
    "\n",
    "def count_adjectives(doc):\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    return len(adjectives)\n",
    "\n",
    "def count_unique_words(doc):\n",
    "    unique_words = set(token.text.lower() for token in doc if token.is_alpha)\n",
    "    return len(unique_words)\n",
    "\n",
    "swear_words = [\n",
    "    \"fuck\", \"shit\", \"bitch\", \"asshole\", \"dick\", \"cunt\", \"bastard\",\n",
    "    \"motherfucker\", \"cock\", \"piss\", \"twat\", \"ass\", \"damn\", \"hell\",\n",
    "    \"bollocks\", \"arsehole\", \"wanker\", \"prick\", \"slut\", \"whore\", \"fucking\", \"mufuckas\", \"fuckin\", \"motherfuckin\",\n",
    "    \"muthafuckin\", \"nigga\", \"niggas\"\n",
    "] \n",
    "def count_swear_words(text):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    swear_word_count = sum(word in swear_words for word in words)\n",
    "    return swear_word_count\n",
    "\n",
    "pop_words = ['love', 'kiss', 'baby', 'dance', 'oh']\n",
    "\n",
    "def count_pop_words(text):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    pop_words_count = sum(word in pop_words for word in words)\n",
    "    return pop_words_count\n",
    "\n",
    "def sentiment_score_extractor(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def unique_word_density(text):\n",
    "    return len(set(text.split())) / len(text.split()) if len(text.split()) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4744d4e76b5b1bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:16:04.319331Z",
     "start_time": "2024-04-07T15:06:32.449921Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39053/39053 [09:31<00:00, 68.32it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Intro: Method Man w/ sample] + (Sunny valentine). We got butter (8X). (The gun'll go the gun'll go.... The gun'll go...). [Raekwon]. Aiyo one thing for sure keep you of all. Keep a nice crib fly away keep to the point. Keep niggaz outta ya face who snakes. Keep bitches in they place keep the mac in a special place. Keep moving for papes keep cool keep doing what you doing. Keep it fly keep me in the crates. Cuz I will erase shit on the real note you'se a waste. It's right here for you I will lace you. Rip you and brace you put a nice W up on ya face. Word to mother you could get chased. It's nothing to taste blood on a thug if he gotta go. All I know is we be giving grace. This is a place from where we make tapes. We make 'em everywhere still in all we be making base. Y'all be making paste these little niggaz they be making shapes. Our shit is art yours is traced. [Chorus: Sunny Valentine]. This is the way that we rolling in the streets. You know when we roll we be packing that heat. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go.... [Method Man]. This is Poverty Island man these animals don't run. Slums where the ambulance don't come. Who got the best base? Fiends waiting to smoke some. Approach something ask him where he getting that coke from. My dudes hug blocks like samurai shogun. Cuz no V and no ones equalling no fun. Who want a treat they know huh? Body to go numb. My woman need funds plus her hair and her toes done. It is what it is though you fuck with the kid flow. That make it hard to get dough the harder to get gold. Harder the piff blow harder when it snow. The pinky and the wrist glow this here what we live for. Get gwop then get low but first thought. We gotta get the work off the gift and the curse boss. Yeah see I'm the shit yo the dirt in the fit no. Hustling from the get-go the motto is get more. [Chorus]. [Masta Killa]. We was quiet flashy brothers strapped all along. With the dirty .38 long twelve hour shift gate. Took case state to state you think he won't hold his weight?. Put ya money on the plate and watch it get scrapped. We get ape up in that club off that juice and Henn. And it's a no win situation fucking with them. You mean like Ewing at the front at the rim finger roll a Dutch. Million dollar stages touched techs gauges bust. Trust no one the lone shogun rugged Timb boot stomper. Damaging lyrical mass destruction launcher. Nothing can calm the quakeage when I break kid. Peace to my brothers up north doing state bids. [Chorus]. [Chorus 2: Sunny Valentine]. Whoa... this is the way we be rolling in the club. You know when we roll we be packing .32 snubs. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go the gun'll go the gun'll go. The gun'll go the gun'll go the gun'll go the gun'll go. [Outro: sample to fade]. We got butter...,\n",
       " [Sean Paul:]. Aye. It's Sean Paul 'long side. The mandem called Jay Sean. Fi di gal dem. Tellin' 'em again what we tell 'em. [Jay Sean:]. Pass me a drink to the left yeah. Said her name was Delilah. And I'm like \"you should come my way\". I already surrender. Damn girl that body's fire. You gon' remember my name. (She should give it up definite). You need it. I need it. We can jump in the deep end. I wanna get lost in your love. I just wanna be close to you. (Just wanna I just wanna). And do all the things you want me to. I just wanna be close to you. (I just wanna I just wanna). And show you the way I feel. You make my love go. You make my love go. You make my love go. In the morning we gon' do it again wake up. I'mma do it like we just broke up and made up. Get up on top of me and work up a sweat work up a sweat. See we can do it any type of way that you want. I'm thinking maybe you're the right kind of wrong. I'm saying baby you won't ever forget my love. You need it. I need it. We can jump in the deep end. I wanna get lost in your love. I just wanna be close to you. (Just wanna I just wanna). And do all the things you want me to. I just wanna be close to you. (I just wanna I just wanna). And show you the way I feel. You make my love go. You make my love go. You make my love go. [Sean Paul:]. Girl mi wan' figure hundred hundred and fifty. Love how you move you know that I'm with it. Perfect size I know that you fit it. Just let me hit it you know mi not quit it. Pon di Dl like Cassie and Diddy. Mi na wound a mi watch we like Sin City. Full time mi run da ting mi tall legend. If you don't come gimme dat would I be offended my girl. Come here down wan' see something me want in life and then waste time. A you a mi pree every day baby full time when ya de pon on mi mind. So mi wine if you give it to me baby girl so we can play. Stick to the ting now I am your king my girl this is what we say. [Jay Sean:]. I just wanna be close to you. (Just wanna I just wanna). And do all the things you want me to. I just wanna be close to you. (I just wanna I just wanna). And show you the way I feel. You make my love go. You make my love go. You make my love go,\n",
       " You've changed your tune. many times since we've met. But I'll always recognize you. You are part of me. I feel you whether in Jamaica or in the Angel city. You're such a gift to me. Uuuuh. Chorus. Hey you. I gotta tell you my long time friend. I think of all those years you saw me through tears. and the good times that we spend. Hey you. You're my constant companion. You always let me explain just what I'm saying. and we've just begun. Ooooh. I've got one wish for this music to be an uplift. And I need an uplift to deal. There are few songs all the people can sing along one song. Come along and sing it for real. Uuuuh. Chorus. Hey you. I gotta tell you my long time friend. I think of all those years you saw me through tears. And the good times that we spend. Hey you. You're my constant companion. You always let me explain just what I'm saying. and we've just begun]. ooooh. Bridge. As I wander around town to town. Lost and found. When so many others come and go. (come and go). The sweetness (sweetness). Pulls me through. Chorus. Hey you. I gotta tell you my long time friend. I think of all those years you saw me through tears. and the good times that we spend. Hey you. You're my constant companion. You always let me explain just what I'm saying. and we've just begun. Hey you. You never turned your back on me. (ooooh). when I gave up on myself. Hey you. You never turned your back on me. (ooooh). when I gave upon myself. Hey you. What would the world be like. without you around. Music. You're my constant companion. oooooh]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list(tqdm(nlp.pipe(df_train_base[\"Lyrics\"], batch_size=20, n_process=cpu_count-1), total=len(df_train_base[\"Lyrics\"])))\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5b6a0f28cf32a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:17:12.927392Z",
     "start_time": "2024-04-07T15:16:04.379068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3354/3354 [01:08<00:00, 48.96it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[I was nineteen when I came to town they called it the Summer of Love. They were burning babies burning flags. The hawks against the doves. I took a job in the steamie down on Cauldrum Street. And I fell in love with a laundry girl who was working next to me. Oh she was a rare thing fine as a bee's wing. So fine a breath of wind might blow her away. She was a lost child oh she was running wild. She said \"As long as there's no price on love I'll stay.. And you wouldn't want me any other way\". Brown hair zig-zag around her face and a look of half-surprise. Like a fox caught in the headlights there was animal in her eyes. She said \"Young man oh can't you see I'm not the factory kind. If you don't take me out of here I'll surely lose my mind\". Oh she was a rare thing fine as a bee's wing. So fine that I might crush her where she lay. She was a lost child she was running wild. She said \"As long as there's no price on love I'll stay.. And you wouldn't want me any other way\". We busked around the market towns and picked fruit down in Kent. And we could tinker lamps and pots and knives wherever we went. And I said that we might settle down get a few acres dug. Fire burning in the hearth and babies on the rug. She said \"Oh man you foolish man it surely sounds like hell.. You might be lord of half the world you'll not own me as well\". Oh she was a rare thing fine as a bee's wing. So fine a breath of wind might blow her away. She was a lost child oh she was running wild. She said \"As long as there's no price on love I'll stay.. And you wouldn't want me any other way\". We was camping down the Gower one time the work was pretty good. She thought we shouldn't wait for the frost and I thought maybe we should. We was drinking more in those days and tempers reached a pitch. And like a fool I let her run with the rambling itch. Oh the last I heard she's sleeping rough back on the Derby beat. White Horse in her hip pocket and a wolfhound at her feet. And they say she even married once a man named Romany Brown. But even a gypsy caravan was too much settling down. And they say her flower is faded now hard weather and hard booze. But maybe that's just the price you pay for the chains you refuse. Oh she was a rare thing fine as a bee's wing. And I miss her more than ever words could say. If I could just taste all of her wildness now. If I could hold her in my arms today. Well I wouldn't want her any other way,\n",
       " Your coat and hat are gone. I've really can't look at your little empty shelf. A ragged teddy bear. It feels like we never had a chance. Don't look me in the eye. We lay in each others arms. But the room is just an empty space. I guess we've lived it out. Something in the air. We smiled to fast then can't think of a thing to say. Lived with the best times. Left with the worst. I've danced with you too long. Nothing left to save. Let's take what we can. I know you hold your head up high. We've raced for the last time. A place of no return. And there's is something in the air. Something in my eye. I've dance with you too long (yeah). Something in the air. Something in my eye. Abracadoo I loose you. We can't avoid the clash. The big mistake. Now we're gonna pay and pay. The sentence of our lives. Can't believe I'm asking you to go. We used what we could. To get the things we want. But we lost each other on the way. I guess you know I never wanted anyone more than you. Lived all our best times. Left with the worst. I've danced with you to long. Say what you will. There's something in the air. Raced for the last time. Well I know you'll hold your head up high. But it's nothing we have to say. There's nothing in our eyes. But there's something in the air. Something in my eye. I've danced with you too long. There's something I have to say. There's something in the air. Something in my eye. Do do do do. Do do do do. I've danced with you too long. Do do do do. Do do do do. Danced with you too long. Do do. Danced with you too long. Something in the air. Something in the air. Do do do do.,\n",
       " I met her at the 'Burger King'. We fell in love by the soda machine. So we took the car down town. The kids were hanging out all around. Then we went down to the Coney Island. On the coaster and around again. And no one's gonna ever tear us apart. 'Cause she's my sweetheart. All right oh yea. Oh oh I love her so. Oh oh I love her so. Oh oh I love her so. Oh (2x). Hanging out on a night like this. I'm gonna give her a great big kiss. I'm gonna make her mine. And everything's gonna be real fine. I met her at the 'Burger King'. Fell in love by the soda machine. So we took the car down town. The kids were hanging out all around. Then we went down to the Coney Island. On the coaster and around again. And no one's gonna ever tear us apart. 'Cause she's my sweetheart. All right oh yea. Oh oh I love her so. Oh oh I love her so. Oh oh I love her so. Oh oh (3x)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_test = list(tqdm(nlp.pipe(df_test_base[\"Lyrics\"], batch_size=20, n_process=cpu_count-1), total=len(df_test_base[\"Lyrics\"])))\n",
    "docs_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c289a816fb2afce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:27:36.735496Z",
     "start_time": "2024-04-07T15:27:36.727860Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(df, docs_, vectorized):\n",
    "    df_processed = df.copy()\n",
    "    df_processed[\"Lyrics\"] = df_processed[\"Lyrics\"].apply(remove_punctuation)\n",
    "    df_processed[\"Lyrics\"] = df_processed[\"Lyrics\"].apply(remove_stopwords, \n",
    "                                                      stopwords = en_stopwords, \n",
    "                                                      tokenizer = WordPunctTokenizer())\n",
    "    # df_processed[\"nb_words\"] = df_processed[\"Lyrics\"].apply(lambda text: len(text.split()))\n",
    "    df_processed[\"doc_length\"] = df_processed[\"Lyrics\"].apply(lambda text: len(text))\n",
    "    df_processed['swear_words_count'] = df_processed['Lyrics'].apply(count_swear_words)\n",
    "    \n",
    "    df_processed['pop_words_count'] = df_processed['Lyrics'].apply(count_pop_words)\n",
    "    # df_processed[\"avg_word_length\"] = df_processed[\"Lyrics\"].apply(average_word_length)\n",
    "    \n",
    "    df_processed['sentiment_score'] = df_processed['Lyrics'].apply(sentiment_score_extractor)\n",
    "    \n",
    "    df_processed['unique_word_density'] = df_processed['Lyrics'].apply(unique_word_density)\n",
    "    \n",
    "    df_processed['adj_count'] = [count_adjectives(doc) for doc in docs_]\n",
    "    # df_processed[\"avg_sentence_length\"] = [sum(len(sent) for sent in doc.sents) / len(list(doc.sents)) for doc in docs_]\n",
    "    \n",
    "    df_processed['unique_word_count'] = [count_unique_words(doc) for doc in docs_]\n",
    "    \n",
    "    df_processed = pd.concat([df_processed, vectorized], axis=1)\n",
    "    \n",
    "    # df_processed = pd.concat([df_processed, c_vectorized], axis=1)\n",
    "    \n",
    "    # df_processed = df_processed.drop(columns=\"Lyrics\")\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baab93ca389a90e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:28:21.118763Z",
     "start_time": "2024-04-07T15:27:38.126489Z"
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df_train_base.copy()\n",
    "df_processed = pre_process(df_processed, docs, df_train_vectorized)\n",
    "df_test_processed = pre_process(df_test_base.copy(), docs_test, df_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "733a9801d64f68d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:18:24.436695Z",
     "start_time": "2024-04-07T15:18:24.404232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Genre</th>\n",
       "      <th>doc_length</th>\n",
       "      <th>swear_words_count</th>\n",
       "      <th>pop_words_count</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>unique_word_density</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intro method man w sample sunny valentine got ...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>1602</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052575</td>\n",
       "      <td>0.737226</td>\n",
       "      <td>24</td>\n",
       "      <td>253</td>\n",
       "      <td>-0.136609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027427</td>\n",
       "      <td>0.102448</td>\n",
       "      <td>-0.190991</td>\n",
       "      <td>-0.315929</td>\n",
       "      <td>0.167530</td>\n",
       "      <td>-0.343603</td>\n",
       "      <td>-0.142754</td>\n",
       "      <td>-0.543318</td>\n",
       "      <td>-0.231015</td>\n",
       "      <td>0.698872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sean paul aye sean paul long mandem called jay...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.293681</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>17</td>\n",
       "      <td>144</td>\n",
       "      <td>-0.566853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176990</td>\n",
       "      <td>-0.680539</td>\n",
       "      <td>0.437265</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>0.325816</td>\n",
       "      <td>-0.393507</td>\n",
       "      <td>0.217912</td>\n",
       "      <td>0.085732</td>\n",
       "      <td>0.803546</td>\n",
       "      <td>0.631783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youve changed tune times weve met ill recogniz...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>775</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.462121</td>\n",
       "      <td>14</td>\n",
       "      <td>106</td>\n",
       "      <td>0.445194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150421</td>\n",
       "      <td>-0.704043</td>\n",
       "      <td>0.435571</td>\n",
       "      <td>0.133274</td>\n",
       "      <td>0.570402</td>\n",
       "      <td>-0.138810</td>\n",
       "      <td>0.194687</td>\n",
       "      <td>-0.795902</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>-0.515650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>got js rolled got drinks poured bought bottles...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>1189</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>10</td>\n",
       "      <td>142</td>\n",
       "      <td>0.428760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204813</td>\n",
       "      <td>-0.295936</td>\n",
       "      <td>0.591472</td>\n",
       "      <td>0.269355</td>\n",
       "      <td>0.139109</td>\n",
       "      <td>-1.660812</td>\n",
       "      <td>-0.344285</td>\n",
       "      <td>-0.867631</td>\n",
       "      <td>-0.044837</td>\n",
       "      <td>0.059782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look im standing naked dont want sex scream lo...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>436</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.376923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>-0.530487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226589</td>\n",
       "      <td>-1.219079</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>0.031436</td>\n",
       "      <td>-0.121683</td>\n",
       "      <td>0.099747</td>\n",
       "      <td>0.525335</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.575657</td>\n",
       "      <td>0.224653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lyrics    Genre  doc_length  \\\n",
       "0  intro method man w sample sunny valentine got ...  Hip Hop        1602   \n",
       "1  sean paul aye sean paul long mandem called jay...      Pop        1010   \n",
       "2  youve changed tune times weve met ill recogniz...     Rock         775   \n",
       "3  got js rolled got drinks poured bought bottles...  Hip Hop        1189   \n",
       "4  look im standing naked dont want sex scream lo...     Rock         436   \n",
       "\n",
       "   swear_words_count  pop_words_count  sentiment_score  unique_word_density  \\\n",
       "0                  5                0         0.052575             0.737226   \n",
       "1                  1               16         0.293681             0.505051   \n",
       "2                  0                0         0.137500             0.462121   \n",
       "3                  2                2         0.002126             0.478049   \n",
       "4                  0               14         0.376923             0.500000   \n",
       "\n",
       "   adj_count  unique_word_count         0  ...        90        91        92  \\\n",
       "0         24                253 -0.136609  ... -0.027427  0.102448 -0.190991   \n",
       "1         17                144 -0.566853  ...  0.176990 -0.680539  0.437265   \n",
       "2         14                106  0.445194  ... -0.150421 -0.704043  0.435571   \n",
       "3         10                142  0.428760  ...  0.204813 -0.295936  0.591472   \n",
       "4         10                 78 -0.530487  ...  0.226589 -1.219079  0.298557   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0 -0.315929  0.167530 -0.343603 -0.142754 -0.543318 -0.231015  0.698872  \n",
       "1  0.029490  0.325816 -0.393507  0.217912  0.085732  0.803546  0.631783  \n",
       "2  0.133274  0.570402 -0.138810  0.194687 -0.795902 -0.058599 -0.515650  \n",
       "3  0.269355  0.139109 -1.660812 -0.344285 -0.867631 -0.044837  0.059782  \n",
       "4  0.031436 -0.121683  0.099747  0.525335  0.087373  0.575657  0.224653  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "288b2b122a685a9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:18:30.806916Z",
     "start_time": "2024-04-07T15:18:30.803343Z"
    }
   },
   "outputs": [],
   "source": [
    "text_pipe = Pipeline([\n",
    "    ('selector', TextSelector(key='Lyrics')),\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=6))\n",
    "])\n",
    "\n",
    "# nb_words_pipe = Pipeline([\n",
    "#     ('selector', NumberSelector(key='nb_words')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "unique_word_count_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='unique_word_count')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "swear_words_count_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='swear_words_count')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pop_words_count_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='pop_words_count')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "sentiment_score_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='sentiment_score')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "unique_word_density_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='unique_word_density')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "doc_length_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='doc_length')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "adj_count_pipe = Pipeline([\n",
    "    ('selector', NumberSelector(key='adj_count')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# avg_word_length_pipe = Pipeline([\n",
    "#     ('selector', NumberSelector(key='avg_word_length')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# avg_sentence_length_pipe = Pipeline([\n",
    "#     ('selector', NumberSelector(key='avg_sentence_length')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "feats = FeatureUnion([\n",
    "    ('Lyrics', text_pipe),\n",
    "    # ('nb_words', nb_words_pipe),\n",
    "    # ('doc_length', doc_length_pipe),\n",
    "    ('adj_count', adj_count_pipe),\n",
    "    ('swear_words_count', swear_words_count_pipe),\n",
    "    ('unique_word_count', unique_word_count_pipe),\n",
    "    ('pop_word_count', pop_words_count_pipe),\n",
    "    ('sentiment_score', sentiment_score_pipe),\n",
    "    ('unique_word_density', unique_word_density_pipe),\n",
    "    # ('drop_column', ColumnTransformer([\n",
    "    #     ('drop_lyrics', FunctionTransformer(lambda X: X.drop(columns=['Lyrics']), validate=False), [])\n",
    "    # ], remainder='passthrough'))\n",
    "    # ('avg_word_length', avg_word_length_pipe),\n",
    "    # ('avg_sentence_length', avg_sentence_length_pipe)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dfbde9fc96e817b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:18:31.988567Z",
     "start_time": "2024-04-07T15:18:31.978804Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, X_test, feats, clf):\n",
    "    pipe = Pipeline([\n",
    "        ('feats', feats),\n",
    "        # ('feature_selection', SelectKBest(mutual_info_classif, k=500)),  \n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return pipe, y_pred\n",
    "    \n",
    "def improved_pipeline(feats, X_train, X_test, y_train, y_test, clf):\n",
    "    \"\"\"\n",
    "    Creates a pipeline with the provided feature union and a Random Forest classifier.\n",
    "    Fits the pipeline to the train data and makes a prediction with the test data.\n",
    "    Outputs the fitted pipeline and the accuracy of the prediction.\n",
    "    \"\"\"\n",
    "    pipe, y_pred = predict(X_train, y_train, X_test, feats, clf)\n",
    "    \n",
    "    f1, report, matrix = get_scores(y_pred, y_test)\n",
    "\n",
    "    return pipe, f1, report, matrix, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c29775e150e8d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:18:32.999617Z",
     "start_time": "2024-04-07T15:18:32.988392Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_scores(y_test, y_pred):\n",
    "    # Specify average='macro' for multiclass classification\n",
    "    f1_score_true_label = f1_score(y_test, y_pred, average='weighted')\n",
    "    classification_report_pred = classification_report(y_test, y_pred)\n",
    "    confusion_matrix_pred = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return f1_score_true_label, classification_report_pred, confusion_matrix_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8428fc741ec3725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:28:25.544817Z",
     "start_time": "2024-04-07T15:28:25.533982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Genre</th>\n",
       "      <th>doc_length</th>\n",
       "      <th>swear_words_count</th>\n",
       "      <th>pop_words_count</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>unique_word_density</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intro method man w sample sunny valentine got ...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>1602</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052575</td>\n",
       "      <td>0.737226</td>\n",
       "      <td>24</td>\n",
       "      <td>253</td>\n",
       "      <td>-0.136609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027427</td>\n",
       "      <td>0.102448</td>\n",
       "      <td>-0.190991</td>\n",
       "      <td>-0.315929</td>\n",
       "      <td>0.167530</td>\n",
       "      <td>-0.343603</td>\n",
       "      <td>-0.142754</td>\n",
       "      <td>-0.543318</td>\n",
       "      <td>-0.231015</td>\n",
       "      <td>0.698872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sean paul aye sean paul long mandem called jay...</td>\n",
       "      <td>Pop</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.293681</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>17</td>\n",
       "      <td>144</td>\n",
       "      <td>-0.566853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176990</td>\n",
       "      <td>-0.680539</td>\n",
       "      <td>0.437265</td>\n",
       "      <td>0.029490</td>\n",
       "      <td>0.325816</td>\n",
       "      <td>-0.393507</td>\n",
       "      <td>0.217912</td>\n",
       "      <td>0.085732</td>\n",
       "      <td>0.803546</td>\n",
       "      <td>0.631783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youve changed tune times weve met ill recogniz...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>775</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.462121</td>\n",
       "      <td>14</td>\n",
       "      <td>106</td>\n",
       "      <td>0.445194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150421</td>\n",
       "      <td>-0.704043</td>\n",
       "      <td>0.435571</td>\n",
       "      <td>0.133274</td>\n",
       "      <td>0.570402</td>\n",
       "      <td>-0.138810</td>\n",
       "      <td>0.194687</td>\n",
       "      <td>-0.795902</td>\n",
       "      <td>-0.058599</td>\n",
       "      <td>-0.515650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>got js rolled got drinks poured bought bottles...</td>\n",
       "      <td>Hip Hop</td>\n",
       "      <td>1189</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>10</td>\n",
       "      <td>142</td>\n",
       "      <td>0.428760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204813</td>\n",
       "      <td>-0.295936</td>\n",
       "      <td>0.591472</td>\n",
       "      <td>0.269355</td>\n",
       "      <td>0.139109</td>\n",
       "      <td>-1.660812</td>\n",
       "      <td>-0.344285</td>\n",
       "      <td>-0.867631</td>\n",
       "      <td>-0.044837</td>\n",
       "      <td>0.059782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look im standing naked dont want sex scream lo...</td>\n",
       "      <td>Rock</td>\n",
       "      <td>436</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0.376923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>-0.530487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226589</td>\n",
       "      <td>-1.219079</td>\n",
       "      <td>0.298557</td>\n",
       "      <td>0.031436</td>\n",
       "      <td>-0.121683</td>\n",
       "      <td>0.099747</td>\n",
       "      <td>0.525335</td>\n",
       "      <td>0.087373</td>\n",
       "      <td>0.575657</td>\n",
       "      <td>0.224653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Lyrics    Genre  doc_length  \\\n",
       "0  intro method man w sample sunny valentine got ...  Hip Hop        1602   \n",
       "1  sean paul aye sean paul long mandem called jay...      Pop        1010   \n",
       "2  youve changed tune times weve met ill recogniz...     Rock         775   \n",
       "3  got js rolled got drinks poured bought bottles...  Hip Hop        1189   \n",
       "4  look im standing naked dont want sex scream lo...     Rock         436   \n",
       "\n",
       "   swear_words_count  pop_words_count  sentiment_score  unique_word_density  \\\n",
       "0                  5                0         0.052575             0.737226   \n",
       "1                  1               16         0.293681             0.505051   \n",
       "2                  0                0         0.137500             0.462121   \n",
       "3                  2                2         0.002126             0.478049   \n",
       "4                  0               14         0.376923             0.500000   \n",
       "\n",
       "   adj_count  unique_word_count         0  ...        90        91        92  \\\n",
       "0         24                253 -0.136609  ... -0.027427  0.102448 -0.190991   \n",
       "1         17                144 -0.566853  ...  0.176990 -0.680539  0.437265   \n",
       "2         14                106  0.445194  ... -0.150421 -0.704043  0.435571   \n",
       "3         10                142  0.428760  ...  0.204813 -0.295936  0.591472   \n",
       "4         10                 78 -0.530487  ...  0.226589 -1.219079  0.298557   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0 -0.315929  0.167530 -0.343603 -0.142754 -0.543318 -0.231015  0.698872  \n",
       "1  0.029490  0.325816 -0.393507  0.217912  0.085732  0.803546  0.631783  \n",
       "2  0.133274  0.570402 -0.138810  0.194687 -0.795902 -0.058599 -0.515650  \n",
       "3  0.269355  0.139109 -1.660812 -0.344285 -0.867631 -0.044837  0.059782  \n",
       "4  0.031436 -0.121683  0.099747  0.525335  0.087373  0.575657  0.224653  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d44a7c8647a2067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:20:49.656310Z",
     "start_time": "2024-04-07T15:19:11.035667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7578036309205398\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip Hop       0.79      0.90      0.84      1572\n",
      "         Pop       0.20      0.72      0.32       634\n",
      "        Rock       0.97      0.66      0.78      5605\n",
      "\n",
      "    accuracy                           0.71      7811\n",
      "   macro avg       0.66      0.76      0.65      7811\n",
      "weighted avg       0.87      0.71      0.76      7811\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1407  126   39]\n",
      " [ 110  454   70]\n",
      " [ 263 1652 3690]]\n"
     ]
    }
   ],
   "source": [
    "Y = df_processed[\"Genre\"]\n",
    "X = df_processed.drop(columns=[\"Genre\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "pipeline_model, f1, report, matrix, y_pred_rand = improved_pipeline(feats, X_train, X_test, y_train, y_test, RandomForestClassifier())\n",
    "print(f\"F1 Score: {f1}\\n\")\n",
    "print(f\"Classification Report:\\n {report}\\n\")\n",
    "print(f\"Confusion Matrix:\\n {matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39bede829f422b69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:24:57.544863Z",
     "start_time": "2024-04-07T15:23:15.673932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7608700366287986\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86      1609\n",
      "           1       0.90      0.73      0.80      4688\n",
      "           2       0.44      0.65      0.53      1514\n",
      "\n",
      "    accuracy                           0.75      7811\n",
      "   macro avg       0.72      0.76      0.73      7811\n",
      "weighted avg       0.79      0.75      0.76      7811\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1452   35  122]\n",
      " [ 159 3408 1121]\n",
      " [ 169  356  989]]\n"
     ]
    }
   ],
   "source": [
    "y_train_labeled = y_train.copy()\n",
    "y_train_labeled = y_train_labeled.map({'Hip Hop': 0, 'Rock': 1, 'Pop': 2})\n",
    "y_test_labeled = y_test.copy()\n",
    "y_test_labeled = y_test_labeled.map({'Hip Hop': 0, 'Rock': 1, 'Pop': 2})\n",
    "pipeline_model, f1, report, matrix, y_pred_xgb = improved_pipeline(feats, X_train, X_test, y_train_labeled, y_test_labeled, XGBClassifier(eval_metric='mlogloss'))\n",
    "print(f\"F1 Score: {f1}\\n\")\n",
    "print(f\"Classification Report:\\n {report}\\n\")\n",
    "print(f\"Confusion Matrix:\\n {matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d56eb4c3760bae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:22:38.344219Z",
     "start_time": "2024-04-07T15:22:26.839768Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/itaisoares/workspace/pessoal/batch7-workspace/venv311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7439284569886024\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84      1543\n",
      "           1       0.87      0.73      0.79      4547\n",
      "           2       0.47      0.61      0.53      1721\n",
      "\n",
      "    accuracy                           0.74      7811\n",
      "   macro avg       0.71      0.75      0.72      7811\n",
      "weighted avg       0.76      0.74      0.74      7811\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1396   45  102]\n",
      " [ 154 3308 1085]\n",
      " [ 230  446 1045]]\n"
     ]
    }
   ],
   "source": [
    "y_train_labeled = y_train.copy()\n",
    "y_train_labeled = y_train_labeled.map({'Hip Hop': 0, 'Rock': 1, 'Pop': 2})\n",
    "y_test_labeled = y_test.copy()\n",
    "y_test_labeled = y_test_labeled.map({'Hip Hop': 0, 'Rock': 1, 'Pop': 2})\n",
    "pipeline_model, f1, report, matrix, y_pred_logi = improved_pipeline(feats, X_train, X_test, y_train_labeled, y_test_labeled, LogisticRegression())\n",
    "print(f\"F1 Score: {f1}\\n\")\n",
    "print(f\"Classification Report:\\n {report}\\n\")\n",
    "print(f\"Confusion Matrix:\\n {matrix}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7de2cf5870086dc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:35:01.471352Z",
     "start_time": "2024-04-07T15:32:31.080356Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe, y_pred = predict(X_train=df_processed.drop(columns=\"Genre\") ,\n",
    "                       y_train=df_processed[\"Genre\"].map({'Hip Hop': 0, 'Rock': 1, 'Pop': 2}),\n",
    "                       X_test=df_test_processed,\n",
    "                       feats=feats, \n",
    "                       clf=XGBClassifier(eval_metric='mlogloss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd905af6ea31aca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:36:39.961376Z",
     "start_time": "2024-04-07T15:36:39.947135Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_mapped = pd.DataFrame(y_pred, columns=[\"Genre\"])[\"Genre\"].map({0: 'Hip Hop', 1: 'Rock', 2: 'Pop'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bac053e0d47d10b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:36:45.215099Z",
     "start_time": "2024-04-07T15:36:45.200899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Rock\n",
       "1       Rock\n",
       "2       Rock\n",
       "3        Pop\n",
       "4       Rock\n",
       "        ... \n",
       "3349    Rock\n",
       "3350    Rock\n",
       "3351    Rock\n",
       "3352    Rock\n",
       "3353    Rock\n",
       "Name: Genre, Length: 3354, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39c3163edcc87b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:36:51.731932Z",
     "start_time": "2024-04-07T15:36:51.720159Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame(y_pred_mapped, columns=[\"Genre\"])#.to_csv(\"team_4_13_14.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ada1d0d85c5a56bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:37:00.072690Z",
     "start_time": "2024-04-07T15:37:00.043057Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"team_4_16_36.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dadff69aacaf1c6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T15:22:38.354797Z",
     "start_time": "2024-04-07T15:22:38.345538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre\n",
       "Hip Hop    24.670038\n",
       "Pop        13.236069\n",
       "Rock       10.541357\n",
       "Name: adj_count, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.groupby('Genre')['adj_count'].mean()\n",
    "# unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98488a815df183d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
