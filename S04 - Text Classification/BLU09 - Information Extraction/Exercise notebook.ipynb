{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a3dc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f881c7002cca9b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU09 - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e125f899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:48:36.596880Z",
     "start_time": "2024-03-23T01:48:35.739841Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efa4dd0f5a58872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing needed packages here\n",
    "\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from utils import remove_punctuation, remove_stopwords\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.sha256(json.dumps(str(s)).encode()).hexdigest()\n",
    "\n",
    "cpu_count = int(os.cpu_count()) if os.cpu_count() != None else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a882aedc21b3fc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise notebook you are going to tackle a quite real problem: **Detecting fake news!** Let's create a binary classifier to determine if a piece of news is considered 'reliable' or 'unreliable'. You will start by building some basic features, then go on to build more complex ones, and finally put it all together. You should be able to have a working classifier by the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255edbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b947ffb2b30ff870",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we will be using is the [Fake News](https://www.kaggle.com/c/fake-news/overview) from Kaggle. Each piece of news is either reliable or trustworthy, '0', or unreliable and possibly fake, '1'. First, let's load it and see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c4d9f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:48:37.168802Z",
     "start_time": "2024-03-23T01:48:36.598032Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62fce116d684ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...       Darrell Lucus   \n",
       "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label  \n",
       "id                                                            \n",
       "0   House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...      1  \n",
       "1   Ever get the feeling your life circles the rou...      0  \n",
       "2   Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3   Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4   Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"datasets/fakenews/train.csv\"\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "df = df[:5000]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38f5eb775841d955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see that we have 4 columns that are pretty self-explanatory. Let's drop the author column since we only want to practice our text analysis and drop the title as well for simplicity sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2118e6f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:48:37.172062Z",
     "start_time": "2024-03-23T01:48:37.170260Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4555d789c2c7417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns=[\"author\", \"title\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ec050",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-043e168c017b27d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's also load SpaCy's module with the merged entities (which will come in handy later) and stopwords. `merge_entities` is this function https://spacy.io/api/pipeline-functions#merge_entities. We insert it into the SpaCy pipeline after the NER module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a289d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:48:37.717822Z",
     "start_time": "2024-03-23T01:48:37.173616Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb489232f7a726b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d931a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed6699286cf9b2f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we process the news text with SpaCy. This might take a while depending on your hardware (a break to walk the dog? üê∂)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6313691e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:05.218599Z",
     "start_time": "2024-03-23T01:48:37.718829Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f6782b20750823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [01:27<00:00, 57.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \n",
       " With apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this week‚ÄìFBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clinton‚Äôs email server, the ranking Democrats on the relevant committees didn‚Äôt hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \n",
       " As we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \n",
       " ‚Äî Jason Chaffetz (@jasoninthehouse) October 28, 2016 \n",
       " Of course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of ‚Äúan unrelated case‚Äù‚Äìwhich we now know to be Anthony Weiner‚Äôs sexting with a teenager. But apparently such little things as facts didn‚Äôt matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary wins‚Äìat least two years‚Äô worth, and possibly an entire term‚Äôs worth of them. Apparently Chaffetz thought the FBI was already doing his work for him‚Äìresulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \n",
       " But according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetz‚Äô sins. That aide told Shareblue that his boss and other Democrats didn‚Äôt even know about Comey‚Äôs letter at the time‚Äìand only found out when they checked Twitter. ‚ÄúDemocratic Ranking Members on the relevant committees didn‚Äôt receive Comey‚Äôs letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didn‚Äô receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.‚Äù \n",
       " So let‚Äôs see if we‚Äôve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \n",
       " There has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \n",
       " What it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didn‚Äôt even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesn‚Äôt trample on basic standards of fairness, I don‚Äôt know what does. \n",
       " Granted, it‚Äôs not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetz‚Äô planned fishing expedition. But that doesn‚Äôt mean we can‚Äôt turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \n",
       " Darrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect,\n",
       " Ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [Hillary Clinton remains the big woman on campus in leafy, liberal Wellesley, Massachusetts. Everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way Miss Havisham forever wore that wedding dress.  Speaking of Great Expectations, Hillary Rodham overflowed with them 48 years ago when she first addressed a Wellesley graduating class. The president of the college informed those gathered in 1969 that the students needed ‚Äúno debate so far as I could ascertain as to who their spokesman was to be‚Äù (kind of the like the Democratic primaries in 2016 minus the   terms unknown then even at a Seven Sisters school). ‚ÄúI am very glad that Miss Adams made it clear that what I am speaking for today is all of us ‚Äî  the 400 of us,‚Äù Miss Rodham told her classmates. After appointing herself Edger Bergen to the Charlie McCarthys and Mortimer Snerds in attendance, the    bespectacled in granny glasses (awarding her matronly wisdom ‚Äî  or at least John Lennon wisdom) took issue with the previous speaker. Despite becoming the first   to win election to a seat in the U. S. Senate since Reconstruction, Edward Brooke came in for criticism for calling for ‚Äúempathy‚Äù for the goals of protestors as he criticized tactics. Though Clinton in her senior thesis on Saul Alinsky lamented ‚ÄúBlack Power demagogues‚Äù and ‚Äúelitist arrogance and repressive intolerance‚Äù within the New Left, similar words coming out of a Republican necessitated a brief rebuttal. ‚ÄúTrust,‚Äù Rodham ironically observed in 1969, ‚Äúthis is one word that when I asked the class at our rehearsal what it was they wanted me to say for them, everyone came up to me and said ‚ÄòTalk about trust, talk about the lack of trust both for us and the way we feel about others. Talk about the trust bust.‚Äô What can you say about it? What can you say about a feeling that permeates a generation and that perhaps is not even understood by those who are distrusted?‚Äù The ‚Äútrust bust‚Äù certainly busted Clinton‚Äôs 2016 plans. She certainly did not even understand that people distrusted her. After Whitewater, Travelgate, the vast   conspiracy, Benghazi, and the missing emails, Clinton found herself the distrusted voice on Friday. There was a load of compromising on the road to the broadening of her political horizons. And distrust from the American people ‚Äî  Trump edged her 48 percent to 38 percent on the question immediately prior to November‚Äôs election ‚Äî  stood as a major reason for the closing of those horizons. Clinton described her vanquisher and his supporters as embracing a ‚Äúlie,‚Äù a ‚Äúcon,‚Äù ‚Äúalternative facts,‚Äù and ‚Äúa   assault on truth and reason. ‚Äù She failed to explain why the American people chose his lies over her truth. ‚ÄúAs the history majors among you here today know all too well, when people in power invent their own facts and attack those who question them, it can mark the beginning of the end of a free society,‚Äù she offered. ‚ÄúThat is not hyperbole. ‚Äù Like so many people to emerge from the 1960s, Hillary Clinton embarked upon a long, strange trip. From high school Goldwater Girl and Wellesley College Republican president to Democratic politician, Clinton drank in the times and the place that gave her a degree. More significantly, she went from idealist to cynic, as a comparison of her two Wellesley commencement addresses show. Way back when, she lamented that ‚Äúfor too long our leaders have viewed politics as the art of the possible, and the challenge now is to practice politics as the art of making what appears to be impossible possible. ‚Äù Now, as the big woman on campus but the odd woman out of the White House, she wonders how her current station is even possible. ‚ÄúWhy aren‚Äôt I 50 points ahead?‚Äù she asked in September. In May she asks why she isn‚Äôt president. The woman famously dubbed a ‚Äúcongenital liar‚Äù by Bill Safire concludes that lies did her in ‚Äî  theirs, mind you, not hers. Getting stood up on Election Day, like finding yourself the jilted bride on your wedding day, inspires dangerous delusions.,\n",
       " Why the Truth Might Get You Fired October 29, 2016 \n",
       " The tension between intelligence analysts and political policymakers has always been between honest assessments and desired results, with the latter often overwhelming the former, as in the Iraq War, writes Lawrence Davidson. \n",
       " By Lawrence Davidson \n",
       " For those who might wonder why foreign policy makers repeatedly make bad choices, some insight might be drawn from the following analysis. The action here plays out in the United States, but the lessons are probably universal. \n",
       " Back in the early spring of 2003, George W. Bush initiated the invasion of Iraq. One of his key public reasons for doing so was the claim that the country‚Äôs dictator, Saddam Hussein, was on the verge of developing nuclear weapons and was hiding other weapons of mass destruction. The real reason went beyond that charge and included a long-range plan for ‚Äúregime change‚Äù in the Middle East. President George W. Bush and Vice President Dick Cheney receive an Oval Office briefing from CIA Director George Tenet. Also present is Chief of Staff Andy Card (on right). (White House photo) \n",
       " For our purposes, we will concentrate on the belief that Iraq was about to become a hostile nuclear power. Why did President Bush and his close associates accept this scenario so readily? \n",
       " The short answer is Bush wanted, indeed needed, to believe it as a rationale for invading Iraq. At first he had tried to connect Saddam Hussein to the 9/11 attacks on the U.S. Though he never gave up on that stratagem, the lack of evidence made it difficult to rally an American people, already fixated on Afghanistan, to support a war against Baghdad. \n",
       " But the nuclear weapons gambit proved more fruitful, not because there was any hard evidence for the charge, but because supposedly reliable witnesses, in the persons of exiled anti-Saddam Iraqis (many on the U.S. government‚Äôs payroll ), kept telling Bush and his advisers that the nuclear story was true. \n",
       " What we had was a U.S. leadership cadre whose worldview literally demanded a mortally dangerous Iraq, and informants who, in order to precipitate the overthrow of Saddam, were willing to tell the tale of pending atomic weapons. The strong desire to believe the tale of a nuclear Iraq lowered the threshold for proof . Likewise, the repeated assertions by assumed dependable Iraqi sources underpinned a nationwide U.S. campaign generating both fear and war fever. \n",
       " So the U.S. and its allies insisted that the United Nations send in weapons inspectors to scour Iraq for evidence of a nuclear weapons program (as well as chemical and biological weapons). That the inspectors could find no convincing evidence only frustrated the Bush administration and soon forced its hand. \n",
       " On March 19, 2003, Bush launched the invasion of Iraq with the expectation was that, once in occupation of the country, U.S. inspectors would surely find evidence of those nukes (or at least stockpiles of chemical and biological weapons). They did not. Their Iraqi informants had systematically lied to them. \n",
       " Social and Behavioral Sciences to the Rescue? \n",
       " The various U.S. intelligence agencies were thoroughly shaken by this affair, and today, 13 years later, their directors and managers are still trying to sort it out ‚Äì specifically, how to tell when they are getting ‚Äútrue‚Äù intelligence and when they are being lied to. Or, as one intelligence worker has put it, we need ‚Äú help to protect us against armies of snake oil salesmen. ‚Äù To that end the CIA et al. are in the market for academic assistance. Ahmed Chalabi, head of the Iraqi National Congress, a key supplier of Iraqi defectors with bogus stories of hidden WMD. \n",
       " A ‚Äúpartnership‚Äù is being forged between the Office of the Director of National Intelligence (ODNI), which serves as the coordinating center for the sixteen independent U.S. intelligence agencies, and the National Academies of Sciences, Engineering and Medicine . The result of this collaboration will be a ‚Äú permanent Intelligence Community Studies Board‚Äù to coordinate programs in ‚Äúsocial and behavioral science research [that] might strengthen national security .‚Äù \n",
       " Despite this effort, it is almost certain that the ‚Äúsocial and behavioral sciences‚Äù cannot give the spy agencies what they want ‚Äì a way of detecting lies that is better than their present standard procedures of polygraph tests and interrogations. But even if they could, it might well make no difference, because the real problem is not to be found with the liars. It is to be found with the believers. \n",
       " The Believers \n",
       " It is simply not true, as the ODNI leaders seem to assert, that U.S. intelligence agency personnel cannot tell, more often than not, that they are being lied to. This is the case because there are thousands of middle-echelon intelligence workers, desk officers, and specialists who know something closely approaching the truth ‚Äì that is, they know pretty well what is going on in places like Afghanistan, Iraq, Syria, Libya, Israel, Palestine and elsewhere. Director of National Intelligence James Clapper (right) talks with President Barack Obama in the Oval Office, with John Brennan and other national security aides present. (Photo credit: Office of Director of National Intelligence) \n",
       " Therefore, if someone feeds them ‚Äúsnake oil,‚Äù they usually know it. However, having an accurate grasp of things is often to no avail because their superiors ‚Äì those who got their appointments by accepting a pre-structured worldview ‚Äì have different criterion for what is ‚Äútrue‚Äù than do the analysts. \n",
       " Listen to Charles Gaukel, of the National Intelligence Council ‚Äì yet another organization that acts as a meeting ground for the 16 intelligence agencies. Referring to the search for a way to avoid getting taken in by lies, Gaukel has declared, ‚Äú We‚Äôre looking for truth. But we‚Äôre particularly looking for truth that works. ‚Äù Now what might that mean? \n",
       " I can certainly tell you what it means historically. It means that for the power brokers, ‚Äútruth‚Äù must match up, fit with, their worldview ‚Äì their political and ideological precepts. If it does not fit, it does not ‚Äúwork.‚Äù So the intelligence specialists who send their usually accurate assessments up the line to the policy makers often hit a roadblock caused by ‚Äúgroup think,‚Äù ideological blinkers, and a ‚Äúwe know better‚Äù attitude. \n",
       " On the other hand, as long as what you‚Äôre selling the leadership matches up with what they want to believe, you can peddle them anything: imaginary Iraqi nukes, Israel as a Western-style democracy, Saudi Arabia as an indispensable ally, Libya as a liberated country, Bashar al-Assad as the real roadblock to peace in Syria, the Strategic Defense Initiative (SDI) aka Star Wars, a world that is getting colder and not warmer, American exceptionalism in all its glory ‚Äì the list is almost endless. \n",
       " What does this sad tale tell us? If you want to spend millions of dollars on social and behavioral science research to improve the assessment and use of intelligence, forget about the liars. What you want to look for is an antidote to the narrow-mindedness of the believers ‚Äì the policymakers who seem not to be able to rise above the ideological presumptions of their class ‚Äì presumptions that underpin their self-confidence as they lead us all down slippery slopes. \n",
       " It has happened this way so often, and in so many places, that it is the source of Shakespeare‚Äôs determination that ‚Äúwhat is past, is prelude.‚Äù Our elites play out our destinies as if they have no free will ‚Äì no capacity to break with structured ways of seeing. Yet the middle-echelon specialists keep sending their relatively accurate assessments up the ladder of power. Hope springs eternal.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list(tqdm(nlp.pipe(df[\"text\"], batch_size=20, n_process=cpu_count-1), total=len(df[\"text\"])))\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f722c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-276cdb3161f052ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Overall, the text looks good! Not too many errors, well written... as expected from a news article. Fake news is a very tough, recent problem that is now appearing more and more frequently in the wild. Usually there aren't many ortographic mistakes or slang (as it may happen with spam) since it's coming from news sources that want to appear credible but also clickbaity so that they can profit on that good ad revenue and create distrust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c38a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9871459d26c91a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1. Pipeline\n",
    "\n",
    "With our text processed, let's get a baseline model for our classification problem! Let's use our comfortable _TfidfVectorizer_ to get a simple, fast and trustworthy baseline.\n",
    "\n",
    "Create a function that applies a pipeline to the given train data, makes a prediction for the test data, and returns the accuracy of the prediction. The pipeline should consist of a *TfidfVectorizer* and a *RandomForestClassifier* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07ff301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:05.225354Z",
     "start_time": "2024-03-23T01:50:05.220899Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cf7d1d751604527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_rf_pipeline(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains a TfidfVectorizer + RandomForestClassifier pipeline for the given train data.\n",
    "    Makes a prediction.\n",
    "    Returns the trained pipeline and the accuracy of the prediction.\n",
    "    X_train, y_train: train data, pd.Series\n",
    "    X_test, y_test: test data, pd.Series\n",
    "    \"\"\"\n",
    "    \n",
    "    # pipe = (...)\n",
    "    # pipe.fit(...)\n",
    "    # (...)\n",
    "    # acc =\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pipe = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                   ('classifier', RandomForestClassifier())])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac3eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31eb2fa6359bdecb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the baseline, we will preprocess the text - remove punctuaction and stopwords and tokenize it - then run it through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59999879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:08.409761Z",
     "start_time": "2024-03-23T01:50:05.227585Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c345c9379c620ae2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(remove_punctuation)\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(remove_stopwords, stopwords = en_stopwords, \n",
    "                                                  tokenizer = WordPunctTokenizer())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_processed[\"text\"], df_processed[\"label\"], \n",
    "                                                    test_size=0.2, random_state=42, stratify=df_processed[\"label\"])\n",
    "baseline_model, baseline_acc = tfidf_rf_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# asserts\n",
    "assert isinstance(baseline_model, Pipeline)\n",
    "assert _hash(baseline_model[0]) == 'e68c8e581c16f0d62f3b9cb33a7967b17890e18c1fe819d013181e6714e7a303',\"The\\\n",
    "pipeline parameters are not correct.\"\n",
    "assert _hash(baseline_model[1]) == 'e5fd22909dcc06f7c81407ee302879e41a75675ddfd55fa1ec640ae68a3338d8',\"The\\\n",
    "pipeline parameters are not correct.\"\n",
    "assert np.allclose(baseline_acc, 0.908, 0.1), \"something wrong with the accuracy score. Use the default parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1ec49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecc682a6b13cabf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wow, the accuracy is quite good for such a simple text model! This just proves that a starting trustworthy baseline is all you need. I can't stress enough that it's really important to have a simple first iteration, and afterwards we can add complexity and study which features make sense or not, testing more out of the box solutions. \n",
    "\n",
    "Sometimes, data scientists focus right off the bat on the most complex solutions and a simple one would be enough. Real life problems will obviously achieve lower scores as the datasets are not controlled or cleaned for you but that should not stop you from starting with a simpler and easier solution.\n",
    "\n",
    "Now let's see if we can engineer other features. We will extract information with SpaCy and use it to train the same pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3f611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9acda58125c99fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2. SpaCy Matcher\n",
    "\n",
    "Let's see if we can extract some useful features by using our SpaCy Matcher.\n",
    "\n",
    "#### Q2.a) Simple Matcher\n",
    "\n",
    "You think of some words that could be related with the detection of Fake News. Something starts ringing in your mind about \"propaganda\", \"USA\" and \"fraud\", so you decide to check how many of those words appear in our news articles using the SpaCy Matcher.\n",
    "\n",
    "Use the docs list preprocessed by SpaCy and count the number of occurences of these words in all `Doc`s. The output should be the sum of occurencies in all news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f158c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:09.206951Z",
     "start_time": "2024-03-23T01:50:08.412224Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca5f9a618bc32ee6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words = [\"propaganda\", \"USA\", \"fraud\"]\n",
    "\n",
    "# init the matcher - remember it from the learning notebook\n",
    "# add the patterns of the words. HINT: for a direct match you need a specific pattern (check SpaCy documentation)\n",
    "# count how many matches in all news articles\n",
    "\n",
    "# YOUR CODE HERE\n",
    "counts = []\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "for word in words:\n",
    "    matcher.add(word, [[{'TEXT': word}]])\n",
    "    \n",
    "for idx, doc in enumerate(docs[:]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        counts.append([idx, start, end, span])\n",
    "count = len(counts)\n",
    "\n",
    "# count = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6002d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:09.209353Z",
     "start_time": "2024-03-23T01:50:09.207861Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-57893459c2e5a1ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert _hash(count) == '2357bc4ef05103860befa4a49fd8bbaa1541640845f4b89c1733bb4d6eb7cfcf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374a2f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9a8e7ce0cab87fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.b) POS-Tagging Search\n",
    "\n",
    "Ok, this doesn't look like the way to go, let's look at other theories. You start thinking that fake news might exaggerate on adjectives and adverbs by using exaggerated or over the top descriptions. So you decide to create a feature that counts the number of _Adjectives_ and _Adverbs_ in a piece of news article. The count should be normalized to the word count of the article. Don't forget to exclude the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc30cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:09.213380Z",
     "start_time": "2024-03-23T01:50:09.210135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "ADV\n",
      "VERB\n",
      "PROPN\n",
      "PART\n",
      "NOUN\n",
      "SCONJ\n",
      "PROPN\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "NUM\n",
      "PROPN\n",
      "PROPN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "PROPN\n",
      "PUNCT\n",
      "NOUN\n",
      "NOUN\n",
      "PROPN\n",
      "PUNCT\n",
      "ADJ\n",
      "ADP\n",
      "DET\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n",
      "PROPN\n",
      "NOUN\n",
      "PUNCT\n",
      "SPACE\n",
      "ADP\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "PRON\n",
      "DET\n",
      "PROPN\n",
      "PROPN\n",
      "ADP\n",
      "DET\n",
      "PROPN\n",
      "AUX\n",
      "NOUN\n",
      "PUNCT\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n",
      "CCONJ\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "PROPN\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "SCONJ\n",
      "PRON\n",
      "ADV\n",
      "VERB\n",
      "PRON\n",
      "DET\n",
      "ADV\n",
      "PUNCT\n",
      "ADJ\n",
      "NOUN\n",
      "AUX\n",
      "ADV\n",
      "ADV\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "ADP\n",
      "SCONJ\n",
      "SCONJ\n",
      "PROPN\n",
      "VERB\n",
      "PRON\n",
      "ADV\n",
      "PUNCT\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "SCONJ\n",
      "DET\n",
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "NOUN\n",
      "PRON\n",
      "AUX\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "PROPN\n",
      "PART\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "DET\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "PRON\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "ADP\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "NUM\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "SPACE\n",
      "SCONJ\n",
      "PRON\n",
      "ADV\n",
      "VERB\n",
      "PUNCT\n",
      "PROPN\n",
      "VERB\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "CCONJ\n",
      "ADJ\n",
      "VERB\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "CCONJ\n",
      "PROPN\n",
      "NOUN\n",
      "PRON\n",
      "PRON\n",
      "NOUN\n",
      "AUX\n",
      "VERB\n",
      "NOUN\n",
      "PRON\n",
      "AUX\n",
      "ADV\n",
      "VERB\n",
      "ADP\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "SCONJ\n",
      "PRON\n",
      "VERB\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "PART\n",
      "ADV\n",
      "SCONJ\n",
      "DET\n",
      "NOUN\n",
      "VERB\n",
      "ADP\n",
      "PUNCT\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "VERB\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "ADJ\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "PROPN\n",
      "PROPN\n",
      "ADV\n",
      "VERB\n",
      "PRON\n",
      "PUNCT\n",
      "PUNCT\n",
      "DET\n",
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "PRON\n",
      "VERB\n",
      "PART\n",
      "AUX\n",
      "ADJ\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "PUNCT\n",
      "NOUN\n",
      "VERB\n",
      "SPACE\n",
      "PUNCT\n",
      "PROPN\n",
      "PUNCT\n",
      "PROPN\n",
      "PUNCT\n",
      "PROPN\n",
      "SPACE\n",
      "ADV\n",
      "ADV\n",
      "PUNCT\n",
      "PRON\n",
      "ADV\n",
      "VERB\n",
      "SCONJ\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "PROPN\n",
      "AUX\n",
      "ADV\n",
      "VERB\n",
      "SCONJ\n",
      "PRON\n",
      "AUX\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "ADP\n",
      "PUNCT\n",
      "DET\n",
      "ADJ\n",
      "PROPN\n",
      "PRON\n",
      "ADV\n",
      "VERB\n",
      "PART\n",
      "AUX\n",
      "PROPN\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "ADV\n",
      "ADJ\n",
      "ADJ\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "DET\n",
      "PROPN\n",
      "PROPN\n",
      "AUX\n",
      "ADV\n",
      "VERB\n",
      "PART\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "SCONJ\n",
      "PROPN\n",
      "VERB\n",
      "PUNCT\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "ADV\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PART\n",
      "NOUN\n",
      "ADP\n",
      "PRON\n",
      "PUNCT\n",
      "ADV\n",
      "PROPN\n",
      "VERB\n",
      "DET\n",
      "PROPN\n",
      "AUX\n",
      "ADV\n",
      "VERB\n",
      "PRON\n",
      "NOUN\n",
      "ADP\n",
      "PRON\n",
      "PUNCT\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "SCONJ\n",
      "NOUN\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "SCONJ\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "PRON\n",
      "AUX\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "SPACE\n",
      "CCONJ\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "PROPN\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "AUX\n",
      "AUX\n",
      "AUX\n",
      "DET\n",
      "ADJ\n",
      "ADP\n",
      "NOUN\n",
      "PUNCT\n",
      "NOUN\n",
      "PUNCT\n",
      "DET\n",
      "NOUN\n",
      "VERB\n",
      "PROPN\n",
      "SCONJ\n",
      "PRON\n",
      "NOUN\n",
      "CCONJ\n",
      "ADJ\n",
      "PROPN\n",
      "AUX\n",
      "PART\n",
      "ADV\n",
      "VERB\n",
      "ADP\n",
      "PROPN\n",
      "PART\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "ADV\n",
      "VERB\n",
      "ADP\n",
      "SCONJ\n",
      "PRON\n",
      "VERB\n",
      "PROPN\n",
      "PUNCT\n",
      "PUNCT\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "PROPN\n",
      "PART\n",
      "NOUN\n",
      "ADP\n",
      "ADP\n",
      "DET\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n",
      "ADP\n",
      "NOUN\n",
      "PUNCT\n",
      "DET\n",
      "PROPN\n",
      "PROPN\n",
      "PROPN\n",
      "AUX\n",
      "PUNCT\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "ADP\n",
      "DET\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "PROPN\n",
      "PUNCT\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "CCONJ\n",
      "VERB\n",
      "PRON\n",
      "ADJ\n",
      "PUNCT\n",
      "PUNCT\n",
      "SPACE\n",
      "ADV\n",
      "VERB\n",
      "PRON\n",
      "VERB\n",
      "SCONJ\n",
      "PRON\n",
      "AUX\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "DET\n",
      "PROPN\n",
      "NOUN\n",
      "VERB\n",
      "PROPN\n",
      "CCONJ\n",
      "ADJ\n",
      "PROPN\n",
      "NOUN\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "ADV\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "CCONJ\n",
      "PROPN\n",
      "CCONJ\n",
      "PRON\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "PRON\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADP\n",
      "PRON\n",
      "PUNCT\n",
      "ADV\n",
      "PUNCT\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "PRON\n",
      "VERB\n",
      "ADP\n",
      "ADP\n",
      "PRON\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "SPACE\n",
      "PRON\n",
      "AUX\n",
      "ADV\n",
      "AUX\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "SCONJ\n",
      "PROPN\n",
      "PRON\n",
      "VERB\n",
      "NOUN\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "CCONJ\n",
      "ADJ\n",
      "PROPN\n",
      "PUNCT\n",
      "VERB\n",
      "PRON\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "PRON\n",
      "VERB\n",
      "PRON\n",
      "ADV\n",
      "ADV\n",
      "SCONJ\n",
      "ADV\n",
      "VERB\n",
      "PRON\n",
      "AUX\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "ADV\n",
      "ADV\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "PRON\n",
      "ADV\n",
      "ADV\n",
      "PRON\n",
      "VERB\n",
      "SCONJ\n",
      "PROPN\n",
      "AUX\n",
      "PRON\n",
      "ADJ\n",
      "ADP\n",
      "ADV\n",
      "ADJ\n",
      "CCONJ\n",
      "NOUN\n",
      "PUNCT\n",
      "NOUN\n",
      "PUNCT\n",
      "SPACE\n",
      "PRON\n",
      "PRON\n",
      "AUX\n",
      "VERB\n",
      "PUNCT\n",
      "ADV\n",
      "PUNCT\n",
      "AUX\n",
      "SCONJ\n",
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "PRON\n",
      "VERB\n",
      "PROPN\n",
      "CCONJ\n",
      "PROPN\n",
      "VERB\n",
      "ADP\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "CCONJ\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "ADV\n",
      "VERB\n",
      "DET\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "VERB\n",
      "NOUN\n",
      "PROPN\n",
      "ADP\n",
      "PRON\n",
      "DET\n",
      "NOUN\n",
      "PUNCT\n",
      "SCONJ\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "ADJ\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "PRON\n",
      "VERB\n",
      "PUNCT\n",
      "SPACE\n",
      "VERB\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "PART\n",
      "ADJ\n",
      "SCONJ\n",
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "PRON\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "ADV\n",
      "ADJ\n",
      "NOUN\n",
      "VERB\n",
      "ADP\n",
      "PROPN\n",
      "CCONJ\n",
      "PROPN\n",
      "PUNCT\n",
      "PRON\n",
      "VERB\n",
      "DET\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "CCONJ\n",
      "VERB\n",
      "PROPN\n",
      "DET\n",
      "VERB\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "NUM\n",
      "PUNCT\n",
      "ADV\n",
      "PUNCT\n",
      "DET\n",
      "PROPN\n",
      "PROPN\n",
      "NOUN\n",
      "AUX\n",
      "VERB\n",
      "PRON\n",
      "ADJ\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "PRON\n",
      "AUX\n",
      "PART\n",
      "VERB\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "ADP\n",
      "PRON\n",
      "PUNCT\n",
      "ADV\n",
      "ADV\n",
      "PUNCT\n",
      "PRON\n",
      "AUX\n",
      "DET\n",
      "NOUN\n",
      "NOUN\n",
      "ADP\n",
      "PRON\n",
      "DET\n",
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "CCONJ\n",
      "PRON\n",
      "AUX\n",
      "ADV\n",
      "DET\n",
      "ADJ\n",
      "PROPN\n",
      "PROPN\n",
      "ADP\n",
      "DET\n",
      "PROPN\n",
      "PUNCT\n",
      "ADP\n",
      "PROPN\n",
      "SPACE\n",
      "PROPN\n",
      "AUX\n",
      "DET\n",
      "NUM\n",
      "PUNCT\n",
      "PRON\n",
      "NOUN\n",
      "ADP\n",
      "PROPN\n",
      "PRON\n",
      "VERB\n",
      "PRON\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "DET\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "ADP\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "ADP\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PART\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PRON\n",
      "AUX\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "NOUN\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "ADP\n",
      "PRON\n",
      "PRON\n",
      "AUX\n",
      "AUX\n",
      "VERB\n",
      "ADP\n",
      "NOUN\n",
      "ADV\n",
      "VERB\n",
      "SCONJ\n",
      "PRON\n",
      "VERB\n",
      "DET\n",
      "ADJ\n",
      "NOUN\n",
      "NOUN\n",
      "PUNCT\n",
      "PRON\n",
      "AUX\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "VERB\n",
      "PRON\n",
      "ADP\n",
      "VERB\n",
      "CCONJ\n",
      "VERB\n",
      "ADP\n",
      "PRON\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n",
      "VERB\n",
      "ADV\n",
      "PART\n",
      "VERB\n",
      "PROPN\n",
      "PUNCT\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "for token in docs[0]:\n",
    "    print(token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c170ff4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:09.942646Z",
     "start_time": "2024-03-23T01:50:09.214167Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d048ae4dde4a43cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# HINT: you already have your news text processed (the docs variable),\n",
    "# so you can go over every doc and check if there is any POS Tag which is an ADJ or ADV\n",
    "# to check the POS tag of a token in a doc -----> token.pos_\n",
    "\n",
    "\"\"\"\n",
    "Try it out by running the below code! \n",
    "for token in docs[0]:\n",
    "    print(token.pos_)\n",
    "\"\"\"\n",
    "\n",
    "# Return a list with the number of adjectives and adverbs for every piece of news in docs\n",
    "# Normalize it to the number of words in the given article\n",
    "# nb_adj_adv = [...]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('adjective', [[{'POS': 'ADJ'}]])\n",
    "matcher.add('adverb', [[{'POS': 'ADV'}]])\n",
    "\n",
    "nb_adj_adv = []\n",
    "\n",
    "for doc in docs:\n",
    "    adj_adv_count = sum(1 for token in doc if token.pos_ in ['ADJ', 'ADV'] and not token.is_punct)\n",
    "    word_count = sum(1 for token in doc if not token.is_punct)\n",
    "    normalized_count = adj_adv_count / word_count if word_count > 0 else 0\n",
    "    nb_adj_adv.append(normalized_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4618744a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:09.945849Z",
     "start_time": "2024-03-23T01:50:09.943611Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3961c5c5cb9b18aa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(nb_adj_adv) == list, \"The result should be a list with just 1 dimension.\"\n",
    "assert len(nb_adj_adv) == 5000, \"The length of the result list is wrong.\\\n",
    "You should have a count for every news article.\"\n",
    "np.testing.assert_almost_equal(np.mean(nb_adj_adv), 0.1, decimal=1, err_msg='The result is not correct.')\n",
    "np.testing.assert_almost_equal(np.sum(nb_adj_adv), 528, decimal=0, err_msg='The result is not correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be4f20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa92a0511c909523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's add this feature to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08fa3004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:09.948310Z",
     "start_time": "2024-03-23T01:50:09.946611Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-820cb2d992a833a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed[\"nb_adj_adv\"] = nb_adj_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54363f2d61151f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.c) Adjectivized proper nouns\n",
    "\n",
    "Another theory that might be worth testing is that adjectives with proper nouns are often used in this kind of news to induce sentiments towards people or organization. You want to extract proper nouns preceeded by adjectives to maybe use in a later analysis.\n",
    "\n",
    "Create a `Matcher` to search for adjective + proper noun. Count the number of occurences of each and output the 10 most common as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a89fe123",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:10.778837Z",
     "start_time": "2024-03-23T01:50:09.948933Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a5547e1de72b597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('former President', 99),\n",
       " ('eastern Aleppo', 76),\n",
       " ('many Americans', 74),\n",
       " ('most Americans', 49),\n",
       " ('northern Syria', 49),\n",
       " ('Russian President', 40),\n",
       " ('former Secretary', 38),\n",
       " ('east Aleppo', 31),\n",
       " ('super PAC', 29),\n",
       " ('congressional Republicans', 24)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll reset the matcher for you\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# pattern = [...] to find adjectives followed by proper nouns\n",
    "# matcher.add(\"\", pattern)\n",
    "\n",
    "# for doc in docs:\n",
    "# do matches and save the text in a list\n",
    "\n",
    "# count the number of times the same expression appears in the list (hint: remember the dictionary solution...)\n",
    "# take the top 10 of the counter\n",
    "# the result will be a list of tuples of the form (count, expression)\n",
    "\n",
    "# most_common_adj_propn = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"PROPN\"}]  # Adjective followed by a proper noun\n",
    "matcher.add(\"ADJ_PROPN_PATTERN\", [pattern])\n",
    "\n",
    "# Find matches\n",
    "matches_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # The matched span\n",
    "        matches_list.append(str(span))\n",
    "\n",
    "# Count occurrences and get the 10 most common\n",
    "matches_counter = Counter(matches_list)\n",
    "most_common_adj_propn = matches_counter.most_common(10)\n",
    "\n",
    "# The result will be a list of tuples (expression, count)\n",
    "most_common_adj_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6de656a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:10.781246Z",
     "start_time": "2024-03-23T01:50:10.779685Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a55276f7a5cb1f58",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(most_common_adj_propn) == list, \"the output is not a list\"\n",
    "assert len(most_common_adj_propn) == 10, \"It should be the top 10!\"\n",
    "\n",
    "assert _hash(most_common_adj_propn) == '0b6595146c52b5f1ce86ed96d3390418babe3c5da1497e2c97ea97cf1a830387', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79678441",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5de1f6e3a47a8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at the 10 most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18b06dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:10.783791Z",
     "start_time": "2024-03-23T01:50:10.781981Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a03eaf795b36ce50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('former President', 99),\n",
       " ('eastern Aleppo', 76),\n",
       " ('many Americans', 74),\n",
       " ('most Americans', 49),\n",
       " ('northern Syria', 49),\n",
       " ('Russian President', 40),\n",
       " ('former Secretary', 38),\n",
       " ('east Aleppo', 31),\n",
       " ('super PAC', 29),\n",
       " ('congressional Republicans', 24)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_adj_propn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7e98e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7ba7a85acd3ab24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The counts seem to be too low to use these terms as features. Maybe running a vectorizer on all the results could work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f299d11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13de41c333217acd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.d) Objects of preposition\n",
    "Objects of the sentences could indicate something. For instance, 'NGO financed by Soros' is more likely to appear in fake news than 'NGO financed by UNESCO'. Both objects in these sentences are objects of preposition (hint: SpaCy has a dependency label for this).\n",
    "\n",
    "Create a `Matcher` to search for objects of preposition which are nouns. Again, count the number of occurences of each and output the 10 most common as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ea459a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:12.839471Z",
     "start_time": "2024-03-23T01:50:10.784440Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d047424aca69651a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# I'll reset the matcher for you\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# pattern = [...] to find objects of preposition that are nouns\n",
    "# hint: you need to use dependency and POS labels\n",
    "# matcher.add(\"\", pattern)\n",
    "\n",
    "# for doc in docs:\n",
    "# do matches and save the text in a list\n",
    "\n",
    "# count the number of times the same expression appears in the list (hint: remember the dictionary solution...)\n",
    "# take the top 10 of the counter\n",
    "# the result will be a list of tuples of the form (count, expression)\n",
    "\n",
    "# most_common_pobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Define the pattern for nouns (since we can't directly match dependencies with Matcher)\n",
    "pattern = [{\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"NOUN_PATTERN\", [pattern])\n",
    "\n",
    "# Initialize a list to hold matches that are objects of prepositions\n",
    "pobj_matches = []\n",
    "\n",
    "# Iterate through documents\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        # Get the span for the current match\n",
    "        span = doc[start:end]\n",
    "        # Check if the span's root token has 'pobj' as its dependency label\n",
    "        if span.root.dep_ == 'pobj':\n",
    "            pobj_matches.append(span.text)\n",
    "\n",
    "# Count the occurrences of each match and get the top 10\n",
    "pobj_counter = Counter(pobj_matches)\n",
    "most_common_pobj = pobj_counter.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "938f090a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:12.842091Z",
     "start_time": "2024-03-23T01:50:12.840461Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dd2eee9295599d16",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(most_common_pobj)== list, \"the output is not a list\"\n",
    "assert len(most_common_pobj) == 10, \"It should be the top 10!\"\n",
    "\n",
    "assert _hash(most_common_pobj) == 'eb1416f528dcaa1f08b3c1f3460e2079f8baedb953f8ca810347adbf2d34e52a', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f44647",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b9f1ddf5f7c7ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This time the counts are higher and might be more interesting for a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b568accf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:12.844646Z",
     "start_time": "2024-03-23T01:50:12.842743Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53fa28fe45c63cfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('people', 2749),\n",
       " ('time', 2468),\n",
       " ('world', 1642),\n",
       " ('country', 1558),\n",
       " ('election', 1181),\n",
       " ('campaign', 1144),\n",
       " ('way', 1108),\n",
       " ('government', 1013),\n",
       " ('state', 999),\n",
       " ('life', 937)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_pobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98e7c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad6aa5c72ba1a3e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q2.e) Verbs with direct objects\n",
    "As the last point, you decide to look at verbs with direct objects. These should indicate actions taken towards something or someone. This one can be done without a Matcher.\n",
    "\n",
    "Search for verbs with direct objects which are not pronouns. This time it's a bit trickier - you need to look at the [parse tree](https://spacy.io/usage/linguistic-features#navigating) because the object does not necessarily come right after the verb. Lemmatize both the verb and the object and count the occurences of the lemmatized verb and direct object separated by a space, like this : 'verb_lemma dobj_lemma'. Don't forget to exclude objects that are pronouns.\n",
    "\n",
    "Again, output the 10 most common as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e76d62ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:54:26.725988Z",
     "start_time": "2024-03-23T01:54:25.906262Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9be3562d21acdf58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('take place', 380),\n",
       " ('do thing', 205),\n",
       " ('play role', 196),\n",
       " ('tell reporter', 174),\n",
       " ('kill people', 165),\n",
       " ('win election', 156),\n",
       " ('make decision', 153),\n",
       " ('have right', 149),\n",
       " ('make sense', 141),\n",
       " ('take action', 141)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hint: you need to use the parse tree and the dependency and POS labels\n",
    "\n",
    "# for doc in docs:\n",
    "# do matches and save the text in a list\n",
    "\n",
    "# count the number of times the same expression appears in the list (hint: remember the dictionary solution...)\n",
    "# take the top 10 of the counter\n",
    "# the result will be a list of tuples of the form (count, expression)\n",
    "\n",
    "# most_common_dobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Initialize a counter for verb-direct object pairs\n",
    "verb_dobj_counter = Counter()\n",
    "\n",
    "\n",
    "# Iterate over each document\n",
    "for doc in docs:\n",
    "    # Iterate over each token in the document\n",
    "    for token in doc:\n",
    "        # Check if the token is a verb\n",
    "        if token.pos_ == 'VERB':\n",
    "            # Now, instead of looking at siblings, we explore the children in the parse tree\n",
    "            for child in token.children:\n",
    "                # Check if the child is a direct object and not a pronoun\n",
    "                if child.dep_ == 'dobj' and child.pos_ != 'PRON':\n",
    "                    # Lemmatize the verb and the direct object, creating a combined expression\n",
    "                    expression = f\"{token.lemma_} {child.lemma_}\"\n",
    "                    # Update the counter with this expression\n",
    "                    verb_dobj_counter[expression] += 1\n",
    "\n",
    "# Get the 10 most common verb-direct object combinations\n",
    "most_common_dobj = verb_dobj_counter.most_common(10)\n",
    "\n",
    "# Format the result as a list of tuples (expression, count)\n",
    "most_common_dobj = [(count, exp) for exp, count in most_common_dobj]\n",
    "\n",
    "most_common_dobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8593b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:54:27.673190Z",
     "start_time": "2024-03-23T01:54:27.647498Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aa931f62fa91ab35",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The top ten list is not correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(most_common_dobj)\u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe output is not a list\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(most_common_dobj) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt should be the top 10!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _hash(most_common_dobj) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meb869c73f4e54f102e4b7e0b24fe2b058ff941d01522520ed61d1327ffe6b891\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe top ten list is not correct.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The top ten list is not correct."
     ]
    }
   ],
   "source": [
    "assert type(most_common_dobj)== list, \"the output is not a list\"\n",
    "assert len(most_common_dobj) == 10, \"It should be the top 10!\"\n",
    "\n",
    "assert _hash(most_common_dobj) == 'eb869c73f4e54f102e4b7e0b24fe2b058ff941d01522520ed61d1327ffe6b891', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85479cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-849a37fbcedc4049",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Not so many occurencies, but the whole list could be used with a vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b81e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:50:13.909730Z",
     "start_time": "2024-03-23T01:50:13.909722Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-218910800564bd8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_dobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244da23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59afa572e05bc308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3. Feature Unions\n",
    "\n",
    "We're going to create a few more numerical features here, then use them in a feature union pipeline and see if the baseline improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca16469",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692f18d2b61996fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.a) Adding Extra Features\n",
    "\n",
    "There are a few more simple features that we can extract from the dataset to try to enrich our model. Let's add to our dataframe the following features: **number of words in the news article**, **character length of the news article**,  **average word length**, and **average sentence length**. (Remember that we already have the number of adverbs and adjectives.)\n",
    "\n",
    "Use the SpaCy processed `Doc`s for calculating the sentence length and don't forget to exclude punctuation. Use the tokenized text in `df_processed` for everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acb93e53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:53:49.666385Z",
     "start_time": "2024-03-23T01:53:49.298441Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6910256a13fa82fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_processed[\"nb_words\"] = ...\n",
    "# df_processed[\"doc_length\"] = ...\n",
    "# df_processed[\"avg_word_length\"] = ...\n",
    "# df_processed[\"avg_sentence_length\"] = ...\n",
    "\n",
    "\n",
    "## YOUR CODE HERE\n",
    "df_processed[\"doc_length\"] = df_processed['text'].map(len)\n",
    "df_processed[\"nb_words\"] = df_processed['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "df_processed[\"avg_word_length\"] = df_processed['text'].apply(lambda x: sum([len(word) for word in x.split()])/len(x.split()) if len(x.split())>0 else 0 )\n",
    "\n",
    "\n",
    "# Average Sentence Length\n",
    "df_processed['avg_sentence_length'] = [np.mean([len(sentence) for sentence in doc.sents if len(sentence) > 0]) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ff5b354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:53:50.568140Z",
     "start_time": "2024-03-23T01:53:50.465030Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-31106be84628e3c9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 1 decimals\nSomething is wrong with the avg_sentence_length column.\n ACTUAL: 116357.87017801998\n DESIRED: 116678.1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(df_processed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m14636737\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething wrong with the doc_length column.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_almost_equal(np\u001b[38;5;241m.\u001b[39msum(df_processed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_word_length\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m32100.0\u001b[39m, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, err_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSomething is wrong with the avg_word_length column.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_almost_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_sentence_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m116678.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSomething is wrong with the avg_sentence_length column.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/S04_BLU09/lib/python3.10/site-packages/numpy/testing/_private/utils.py:537\u001b[0m, in \u001b[0;36massert_almost_equal\u001b[0;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(desired \u001b[38;5;241m-\u001b[39m actual) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64(\u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39mdecimal)):\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(_build_err_msg())\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 1 decimals\nSomething is wrong with the avg_sentence_length column.\n ACTUAL: 116357.87017801998\n DESIRED: 116678.1"
     ]
    }
   ],
   "source": [
    "assert df_processed.shape == (5000, 7), \"Something wrong about the shape, do you have all columns/rows?\"\n",
    "assert \"nb_words\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"doc_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_word_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_sentence_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "\n",
    "assert np.sum(df_processed[\"nb_words\"]) == 1963935, \"Something wrong with the nb_words column.\"\n",
    "assert np.sum(df_processed[\"doc_length\"]) == 14636737, \"Something wrong with the doc_length column.\"\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_word_length\"]), 32100.0, decimal=1, err_msg='Something is wrong with the avg_word_length column.')\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_sentence_length\"]), 116678.1, decimal=1, err_msg='Something is wrong with the avg_sentence_length column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1e3de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b12d83fd4d036457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Q3.b) Feature Pipelines\n",
    "\n",
    "Let's create a processing _Pipeline_ for every new feature and join them all in a _Feature Union_. For the textual features use the usual _TfidfVectorizer_ with default parameters and for any numerical feature use a _Standard Scaler_. Afterwards, join the features pipelines using a _Feature Union_.\n",
    "\n",
    "Use the following Selector classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d935c549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:53:55.435444Z",
     "start_time": "2024-03-23T01:53:55.426833Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61e270fd37131ec7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from the dataframe to perform additional transformations on\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "726e0bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:53:56.124874Z",
     "start_time": "2024-03-23T01:53:56.114352Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a5c8a9abba2b5e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# text_pipe = Pipeline([...])\n",
    "# nb_adj_adv_pipe = Pipeline([...])\n",
    "# nb_words_pipe = Pipeline([...])\n",
    "# doc_length_pipe = Pipeline([...])\n",
    "# avg_word_length_pipe = Pipeline([...])\n",
    "# avg_sentence_length_pipe = Pipeline([...])\n",
    "# feats = FeatureUnion(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "text_pipe = Pipeline([\n",
    "                ('selector', TextSelector(\"text\")),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ])\n",
    "\n",
    "nb_adj_adv_pipe =  Pipeline([\n",
    "                ('selector', NumberSelector(\"nb_adj_adv\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "nb_words_pipe =  Pipeline([\n",
    "                ('selector', NumberSelector(\"nb_words\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "doc_length_pipe =  Pipeline([\n",
    "                ('selector', NumberSelector(\"doc_length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "avg_word_length_pipe =  Pipeline([\n",
    "                ('selector', NumberSelector(\"avg_word_length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "avg_sentence_length_pipe = Pipeline([\n",
    "                ('selector', NumberSelector(\"avg_sentence_length\")),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "feats = FeatureUnion([('text', text_pipe), \n",
    "                      ('nb_adj_adv', nb_adj_adv_pipe),\n",
    "                      ('nb_words', nb_words_pipe),\n",
    "                      ('doc_length', doc_length_pipe),\n",
    "                      ('avg_word_length', avg_word_length_pipe),\n",
    "                      ('avg_sentence_length', avg_sentence_length_pipe)\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "402bc439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:53:56.962646Z",
     "start_time": "2024-03-23T01:53:56.956059Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-98151e411b73b1f5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert len(feats.transformer_list) == 6, \"Are you creating 6 pipelines? One for each feature?\"\n",
    "for pipe in feats.transformer_list:\n",
    "    \n",
    "    selector = pipe[1][0]\n",
    "    if not (isinstance(selector, TextSelector) or isinstance(selector, NumberSelector)):\n",
    "        raise AssertionError(\"pipeline is wrong, the Selectors should come first.\")\n",
    "        \n",
    "    feature_builder = pipe[1][1]\n",
    "    if not (isinstance(feature_builder, TfidfVectorizer) or isinstance(feature_builder, StandardScaler)):\n",
    "        raise AssertionError(\"pipeline is wrong, the second thing to come should be the Tfidf or the Scaler.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb7c8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8c92614d69d640a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### Q3.c) Feature Union\n",
    "Now let's build our function to use the newly created _Feature Union_ and calculate its performance!\n",
    "\n",
    "Create a function that will apply the improved pipeline to the provided train data, make a prediction and calculate its accuracy. The pipeline should consist of the feature union we created in Q3.b and a RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d431c958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:53:59.322964Z",
     "start_time": "2024-03-23T01:53:59.317291Z"
    },
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb15cc415e6737d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improved_pipeline(feats, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Creates a pipeline with the provided feature union and a Random Forest classifier.\n",
    "    Fits the pipeline to the train data and makes a prediction with the test data.\n",
    "    Outputs the fitted pipeline and the accuracy of the prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # pipe = (...)\n",
    "    # pipe.fit(...)\n",
    "    # (...)\n",
    "    # acc = ...\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pipe = Pipeline([\n",
    "        ('features', feats),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    preds = pipe.predict(X_test)\n",
    "    acc = np.mean(preds == y_test)\n",
    "    \n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04da0430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T01:54:02.356820Z",
     "start_time": "2024-03-23T01:54:00.235578Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-873e3b1b663a7b12",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y = df_processed[\"label\"]\n",
    "X = df_processed.drop(columns=\"label\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "pipeline_model, pipeline_acc = improved_pipeline(feats, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# asserts\n",
    "assert isinstance(pipeline_model, Pipeline)\n",
    "assert _hash(pipeline_model[0]) == '933e9e022884461f96b8dcbda7872290b8fd44f4003fa618ab5ff8d2a952c247', \"The first part of the\\\n",
    "Pipeline is incorrect.\"\n",
    "assert _hash(pipeline_model[1]) == 'e5fd22909dcc06f7c81407ee302879e41a75675ddfd55fa1ec640ae68a3338d8', \"The second part of the\\\n",
    "Pipeline is incorrect.\"\n",
    "assert np.allclose(pipeline_acc, 0.913, rtol=1e-1), \"Something wrong with the accuracy score. Use the default parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e0258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec9f67cb4f0f5b43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this more complex approach we have achieved basically the same performance as our baseline. This might mean a lot of things: our features might have no real revelance to the model (which you can check with feature importances) or we have achieved a plateau and can't improve the score with this technique. \n",
    "\n",
    "Nevertheless it is a good score for this problem and dataset. Regardless the score, you have learnt a lot about _SpaCy_, _Feature Union_ and also learnt that the sky is the limit when creating features. Anything can be a feature really - now good features are a totally different thing that might need more research and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
